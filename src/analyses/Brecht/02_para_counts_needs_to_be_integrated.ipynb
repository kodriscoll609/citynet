{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2e7f309",
   "metadata": {},
   "source": [
    "## Counting Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caf5a970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import unidecode\n",
    "import numpy as np\n",
    "\n",
    "from preprocessing_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a4c6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pars(article):\n",
    "    \"\"\"count number of paragraphs in an article\"\"\"\n",
    "    count = 0 # initialise count at 0\n",
    "\n",
    "    # split article into paragraphs (by using '\\n' as end of paragraph)\n",
    "    paragraphs = article.splitlines()\n",
    "    for paragraph in paragraphs:\n",
    "        # if paragraph empty skip\n",
    "        if not paragraph:\n",
    "            continue\n",
    "\n",
    "        # check if paragraph > 2\n",
    "        length = len(paragraph.split(' '))\n",
    "        if length >= 2:\n",
    "            count += 1\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d057b990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# language = 'fr' #'fr'\n",
    "lang_list = ['en', 'fr']\n",
    "indir = f'../../../../data/' # path to files that are not in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f57f52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average article in enwiki is 1142 words.\n",
      "The enwiki corpus has a total of 580791582 words.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbf021ce7e764db7a1476be40dc6886b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Articles processed:   0%|          | 0/508671 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enwiki consists of 508671 articles. \n",
      "Making up a total of 10436996 paragraphs where a collocation could occur.\n",
      "The average article in frwiki is 1042 words.\n",
      "The frwiki corpus has a total of 286122298 words.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6746e6f73cba4502992b2126a24e7d1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Articles processed:   0%|          | 0/274578 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frwiki consists of 274578 articles. \n",
      "Making up a total of 6005355 paragraphs where a collocation could occur.\n"
     ]
    }
   ],
   "source": [
    "for lang in lang_list:     \n",
    "    # set up df structure\n",
    "    df = pd.DataFrame(columns = ['article_id', 'title', 'text'])\n",
    "    \n",
    "    # path to directory with article .csv files\n",
    "    inputfp = os.path.join(indir, f'{lang}wiki/')\n",
    "    \n",
    "    # loop over csv files\n",
    "    for file in os.listdir(inputfp):\n",
    "        fp = os.path.join(inputfp, file)\n",
    "        df_temp = pd.read_csv(fp)\n",
    "        df = pd.concat([df, df_temp]) # add into one single df\n",
    "        \n",
    "    # loop over df to get article length\n",
    "    word_counts = [len(text.split()) for text in df.text]\n",
    "    print(f\"The average article in {lang}wiki is {round(np.mean(word_counts))} words.\")\n",
    "    print(f\"The {lang}wiki corpus has a total of {sum(word_counts)} words.\")\n",
    "\n",
    "    \n",
    "    # loop over articles\n",
    "    total_count = 0\n",
    "    for article in tqdm(df.text, total = len(df.text), desc = \"Articles processed\"): \n",
    "        total_count += count_pars(article)\n",
    "    \n",
    "    print(f\"{lang}wiki consists of {len(df)} articles. \\nMaking up a total of {total_count} paragraphs where a collocation could occur.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becdd269",
   "metadata": {},
   "source": [
    "enwiki:  \n",
    "Total number of paragraphs where a collocation could occur is: **10,436,996**  \n",
    "Total number of articles: **508,671**  \n",
    "Average article length: **1142**  \n",
    "Total number of words: **580,791,582**  \n",
    "\n",
    "frwiki:  \n",
    "Total number of paragraphs where a collocation could occur is: **6,005,355**  \n",
    "Total number of articles: **274,578**  \n",
    "Average article length: **1042**  \n",
    "Total number of words: **6005355**  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
