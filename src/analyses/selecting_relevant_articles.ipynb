{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10609389",
   "metadata": {},
   "source": [
    "# Clean the Extracted wikidump files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8524718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c9a31",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c02e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for extracting metadata\n",
    "def find_id(string):\n",
    "    \"\"\"function to extract id from article\"\"\"\n",
    "    \n",
    "    id_pattern = 'id=\\\"\\d+\\\"'\n",
    "#     id_float = '[0-9]+'\n",
    "    \n",
    "    short_string = re.findall(id_pattern, string)[0]\n",
    "    quotation_sym = '\"'\n",
    "    start = short_string.index(quotation_sym) + len(quotation_sym)\n",
    "    end = short_string.index(quotation_sym, start + 1)\n",
    "    actual_id = int(short_string[start:end])\n",
    "\n",
    "#     actual_id = re.findall(id_float, short_string)\n",
    "    \n",
    "    return actual_id\n",
    " \n",
    "def find_url(string):\n",
    "    \"\"\"function to extract url from article\"\"\"\n",
    "    \n",
    "    url_pattern = 'https://en\\.wikipedia\\.org/[a-zA-z\\?=\\d]+'\n",
    "    \n",
    "    url = re.findall(url_pattern, string)\n",
    "    \n",
    "    return url[0]\n",
    "\n",
    "def find_title(string):\n",
    "    \"\"\"function to extract title from article\"\"\"\n",
    "    \n",
    "    pattern = 'title=\\\"[^>]+'\n",
    "    short_string = re.findall(pattern, string)[0]\n",
    "    \n",
    "    quotation_sym = '\"'\n",
    "    start = short_string.index(quotation_sym) + len(quotation_sym)\n",
    "    end = short_string.index(quotation_sym, start + 1)\n",
    "    title = short_string[start:end]\n",
    "    \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f0564f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that checks if key words in corpus\n",
    "def list_in_corpus(list_of_words, text_corpus):\n",
    "    inclusion = False\n",
    "    count = 0\n",
    "    for word in list_of_words:\n",
    "        if count < 2: # only compare if inclusion condition has not yet been met\n",
    "            if word in text_corpus: \n",
    "                count += 1 # add 1 every time a city name is in the corpus\n",
    "        else: \n",
    "            pass\n",
    "    if count > 1: \n",
    "        inclusion = True \n",
    "        # thus the corpus is only marked for inclusion if \n",
    "        # at least two cities from the list have been mentioned\n",
    "    return inclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda9abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split dumps into flat list\n",
    "\n",
    "def split_dump(input_dump, key_words, split_pattern = \"</doc>\"):\n",
    "    \"\"\"\n",
    "    splits list of wikidump documents into a flat list of articles\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "            input_dump:    a list of strings\n",
    "            key_words: list of strings, which should be included in the remaining articles\n",
    "            split_pattern: str, optional\n",
    "                string pattern at which the strings \n",
    "                should be split into articles. default = '<\\doc>'\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    article_list = [\n",
    "        article for dump in tqdm(input_dump, total = len(input_dump), desc = \"Progress\") \n",
    "        for article in dump.split(split_pattern)]\n",
    "    \n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce77bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## old list_in_corpus function\n",
    "# def list_in_corpus_dep(list_of_words, text_corpus):\n",
    "#     inclusion = False\n",
    "#     count = 0\n",
    "#     for word in list_of_words:\n",
    "#         if word in text_corpus:\n",
    "#             count += 1 # add 1 every time a city name is in the corpus\n",
    "#         else: \n",
    "#             pass\n",
    "#     if count > 1: \n",
    "#         inclusion = True \n",
    "#         # thus the corpus is only marked for inclusion if \n",
    "#         # at least two cities from the list have been mentioned\n",
    "#     return inclusion, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e920ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dump(dump, key_words, message = True): \n",
    "    \"\"\"extracts titles and ids from articles containing key words and returns as a list\"\"\"\n",
    "    \n",
    "    articles = []\n",
    "    for article in tqdm(dump, total = len(dump), desc = \"Progress\"): \n",
    "            if (list_in_corpus(cities_ls, article)):\n",
    "                try: \n",
    "                    article_id = find_id(article)\n",
    "                    title = find_title(article)\n",
    "                    articles.append((article_id, title, article)) \n",
    "                except: \n",
    "                    pass\n",
    "            else: \n",
    "                pass\n",
    "            \n",
    "    if message: \n",
    "        print(f\"After processing {len(articles)} articles remain, \" \n",
    "              f\"that is {round(((len(articles)/len(dump))*100), 2)}% \"\n",
    "              f\"of the total number of articles ({len(articles)}) in this dump.\")\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c37c758",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc5b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indir = \"../../../enwiki_extracted_inc\" #\"/Volumes/NIJMAN/THESIS/enwiki_extracted\"\n",
    "\n",
    "indir = \"/Volumes/NIJMAN/THESIS/enwiki_extracted\"\n",
    "wikidump = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "fp_list = []\n",
    "\n",
    "for root, dirs, files in os.walk(indir):\n",
    "\n",
    "    for filename in files:\n",
    "        if not filename.startswith(\".\"): \n",
    "            fp = os.path.join(root, filename)\n",
    "#             fp_list.append(fp)\n",
    "            \n",
    "            with open(fp, 'r') as f: \n",
    "                wikidump.append(f.read())\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "print(f\"This took {total}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d61995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(wikidump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93014763",
   "metadata": {},
   "source": [
    "## City List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cities csv\n",
    "fp = '../input/List_of_cities_300k.csv'\n",
    "\n",
    "cities = pd.read_csv(fp, sep=';')\n",
    "\n",
    "# cities = cities[cities.SizeMUA1k >= 300] # only cities with a population of 300.000 or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb08503a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer('\\w+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a01fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all cities\n",
    "cities_ls = [city for city in cities.Mua_Eng]\n",
    "\n",
    "# split combined citynames \n",
    "stopwords = ['a', 'The', 'the', 'A', 'de', 'di', 'en', 'am']\n",
    "cities_ls = [tokenizer.tokenize(city) for city in cities_ls]\n",
    "cities_ls = [city_component for city in cities_ls for city_component in city]\n",
    "cities_ls = [word for word in cities_ls if not word in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b27ab5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e39ab1",
   "metadata": {},
   "source": [
    "## Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16e413fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial = split_dump(wikidump[:1000])\n",
    "# len(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d66f7494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trial_d = process_dump(trial, cities_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98dc3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(trial_d, columns = ['article_id', 'title', 'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4350fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_in_corpus(cities_ls, df.text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f4691",
   "metadata": {},
   "source": [
    "## Dump 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0f6a2d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba74b16f94204ad282d752f47314703e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/8249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract articles from first half of dump\n",
    "wikidump1 = split_dump(wikidump[:round(len(wikidump)/2)], cities_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc10ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikilength1 = len(wikidump1)\n",
    "wikilength1 = 5090764\n",
    "print(wikilength1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wikidump1 = process_dump(wikidump1, cities_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wikilength1 = len(p_wikidump1)\n",
    "# p_wikilength1 = 'nr that should actually go here'\n",
    "print(p_wikilength1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc9ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to .csv\n",
    "dumps = [p_wikidump1, p_wikidump2]\n",
    "i = 0\n",
    "df = pd.DataFrame(dumps[i], columns = ['article_id', 'title', 'text'])\n",
    "outputfp = f'../../../data/enwikidump{i+1}.csv'\n",
    "df.to_csv(outputfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ac1a2",
   "metadata": {},
   "source": [
    "## Dump2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f654b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract articles from second half of dump\n",
    "\n",
    "wikidump2 = split_dump(wikidump, cities_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikilength2 = len(wikidump2)\n",
    "# wikilength2 = nr that it should be\n",
    "\n",
    "print(wikilength2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikilength1 = 5090764 #len(wikidump1)\n",
    "wikilength2 = len(wikidump2)\n",
    "wikilength = wikilength1 + wikilength2\n",
    "\n",
    "print(f\"nr of articles in the enwiki dump is {wikilength}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28675ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wikidump2 = process_dump(wikidump2, cities_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03992c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wikilength2 = len(p_wikidump2)\n",
    "# p_wikilength2 = 'nr that should actually go here'\n",
    "print(p_wikilength2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef309506",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wikilength = p_wikilength1 + p_wikilength2\n",
    "print(p_wikilength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e62f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to .csv\n",
    "dumps = [p_wikidump1, p_wikidump2]\n",
    "i = 1\n",
    "df = pd.DataFrame(dumps[i], columns = ['article_id', 'title', 'text'])\n",
    "outputfp = f'../../../data/enwikidump{i+1}.csv'\n",
    "df.to_csv(outputfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcb77072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumps = [p_wikidump1, p_wikidump2]\n",
    "# for i in range(2): \n",
    "#     df = pd.DataFrame(dumps[i], columns = ['article_id', 'title', 'text'])\n",
    "#     outputfp = f'../../../data/enwikidump{i+1}.csv'\n",
    "#     df.to_csv(outputfp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf347c58",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
