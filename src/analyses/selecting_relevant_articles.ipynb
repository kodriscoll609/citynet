{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10609389",
   "metadata": {},
   "source": [
    "# Select relevant extracted wikidump files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8524718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2c9a31",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17c02e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions for extracting metadata\n",
    "def find_id(string):\n",
    "    \"\"\"function to extract id from article\"\"\"\n",
    "    \n",
    "    id_pattern = 'id=\\\"\\d+\\\"'\n",
    "#     id_float = '[0-9]+'\n",
    "    \n",
    "    short_string = re.findall(id_pattern, string)[0]\n",
    "    quotation_sym = '\"'\n",
    "    start = short_string.index(quotation_sym) + len(quotation_sym)\n",
    "    end = short_string.index(quotation_sym, start + 1)\n",
    "    actual_id = int(short_string[start:end])\n",
    "\n",
    "#     actual_id = re.findall(id_float, short_string)\n",
    "    \n",
    "    return actual_id\n",
    " \n",
    "def find_url(string):\n",
    "    \"\"\"function to extract url from article\"\"\"\n",
    "    \n",
    "    url_pattern = 'https://en\\.wikipedia\\.org/[a-zA-z\\?=\\d]+'\n",
    "    \n",
    "    url = re.findall(url_pattern, string)\n",
    "    \n",
    "    return url[0]\n",
    "\n",
    "def find_title(string):\n",
    "    \"\"\"function to extract title from article\"\"\"\n",
    "    \n",
    "    pattern = 'title=\\\"[^>]+'\n",
    "    short_string = re.findall(pattern, string)[0]\n",
    "    \n",
    "    quotation_sym = '\"'\n",
    "    start = short_string.index(quotation_sym) + len(quotation_sym)\n",
    "    end = short_string.index(quotation_sym, start + 1)\n",
    "    title = short_string[start:end]\n",
    "    \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f0564f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that checks if key words in corpus\n",
    "def list_in_corpus(list_of_words, text_corpus):\n",
    "    inclusion = False\n",
    "    count = 0\n",
    "    for word in list_of_words:\n",
    "        if count < 2: # only compare if inclusion condition has not yet been met\n",
    "            if word in text_corpus: \n",
    "                count += 1 # add 1 every time a city name is in the corpus\n",
    "        else: \n",
    "            pass\n",
    "    if count > 1: \n",
    "        inclusion = True \n",
    "        # thus the corpus is only marked for inclusion if \n",
    "        # at least two cities from the list have been mentioned\n",
    "    return inclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cda9abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to split dumps into flat list\n",
    "\n",
    "def split_dump(input_dump, key_words, split_pattern = \"</doc>\"):\n",
    "    \"\"\"\n",
    "    splits list of wikidump documents into a flat list of articles\n",
    "    \n",
    "        Parameters:\n",
    "        -----------\n",
    "            input_dump:    a list of strings\n",
    "            key_words: list of strings, which should be included in the remaining articles\n",
    "            split_pattern: str, optional\n",
    "                string pattern at which the strings \n",
    "                should be split into articles. default = '<\\doc>'\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    article_list = [\n",
    "        article for dump \n",
    "        in tqdm(input_dump, total = len(input_dump), desc = \"Progress\") \n",
    "        for article in dump.split(split_pattern)]\n",
    "    \n",
    "    return article_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce77bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## old list_in_corpus function\n",
    "# def list_in_corpus_dep(list_of_words, text_corpus):\n",
    "#     inclusion = False\n",
    "#     count = 0\n",
    "#     for word in list_of_words:\n",
    "#         if word in text_corpus:\n",
    "#             count += 1 # add 1 every time a city name is in the corpus\n",
    "#         else: \n",
    "#             pass\n",
    "#     if count > 1: \n",
    "#         inclusion = True \n",
    "#         # thus the corpus is only marked for inclusion if \n",
    "#         # at least two cities from the list have been mentioned\n",
    "#     return inclusion, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e920ee06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dump(dump, key_words, message = True): \n",
    "    \"\"\"extracts titles and ids from articles containing key words and returns as a list\"\"\"\n",
    "    \n",
    "    articles = []\n",
    "    for article in tqdm(dump, total = len(dump), desc = \"Progress\"):\n",
    "        article = unidecode.unidecode(article)\n",
    "        if (list_in_corpus(cities_ls, article)):\n",
    "            try: \n",
    "                article_id = find_id(article)\n",
    "                title = find_title(article)\n",
    "                articles.append((article_id, title, article)) \n",
    "            except: \n",
    "                pass\n",
    "        else: \n",
    "            pass\n",
    "            \n",
    "    if message: \n",
    "        print(f\"After processing {len(articles)} articles remain, \" \n",
    "              f\"that is {round(((len(articles)/len(dump))*100), 2)}% \"\n",
    "              f\"of the total number of articles ({len(articles)}) in this dump.\")\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c37c758",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1bc5b5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load articles from dump\n",
    "indir = \"/Volumes/NIJMAN/THESIS/enwiki_extracted\" # path/to/wikidump_extracted\n",
    "wikidump = [] # initialize empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9d4291",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "fp_list = []\n",
    "\n",
    "for root, dirs, files in os.walk(indir):\n",
    "\n",
    "    for filename in files:\n",
    "        if not filename.startswith(\".\"): \n",
    "            fp = os.path.join(root, filename)\n",
    "#             fp_list.append(fp)\n",
    "            \n",
    "            with open(fp, 'r') as f: \n",
    "                wikidump.append(f.read())\n",
    "\n",
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0\n",
    "print(f\"This took {total}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d61995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(wikidump)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93014763",
   "metadata": {},
   "source": [
    "## City List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "533b6329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cities csv\n",
    "fp = '../input/List_of_cities_300k.csv' # path to csv with city information\n",
    "\n",
    "cities = pd.read_csv(fp, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a01fded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of all cities\n",
    "name_col = 'Mua_Eng' # for french 'Mua_Fr'\n",
    "cities_ls = [city for city in cities[name_col]]\n",
    "\n",
    "# split combined citynames \n",
    "\n",
    "cities_ls = [city.split('-') for city in cities_ls] # splits Muas that are made up of multiple cities e.g. Essen-Oberhausen\n",
    "cities_ls = [city_component for city in cities_ls for city_component in city]\n",
    "cities_ls = [unidecode.unidecode(word) for word in cities_ls] # remove accents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d483a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Paris',\n",
       " 'London',\n",
       " 'Madrid',\n",
       " 'Berlin',\n",
       " 'Milan',\n",
       " 'Barcelona',\n",
       " 'Athens',\n",
       " 'Rome',\n",
       " 'Birmingham',\n",
       " 'Lisbon',\n",
       " 'Naples',\n",
       " 'Katowice',\n",
       " 'Manchester',\n",
       " 'Hamburg',\n",
       " 'Budapest',\n",
       " 'Bucharest',\n",
       " 'Warsaw',\n",
       " 'Stuttgart',\n",
       " 'Vienna',\n",
       " 'Munich',\n",
       " 'Brussels',\n",
       " 'Stockholm',\n",
       " 'Frankfurt',\n",
       " 'Cologne',\n",
       " 'Copenhagen',\n",
       " 'Valencia',\n",
       " 'Turin',\n",
       " 'Glasgow',\n",
       " 'Prague',\n",
       " 'Lyon',\n",
       " 'Sofia',\n",
       " 'Liverpool',\n",
       " 'Porto',\n",
       " 'Seville',\n",
       " 'Dublin',\n",
       " 'Helsinki',\n",
       " 'Amsterdam',\n",
       " 'Rotterdam',\n",
       " 'Dusseldorf',\n",
       " 'Essen',\n",
       " 'Oberhausen',\n",
       " 'Lille',\n",
       " 'Lodz',\n",
       " 'Marseille',\n",
       " 'Antwerp',\n",
       " 'Bilbao',\n",
       " 'Newcastle',\n",
       " 'Krakow',\n",
       " 'Bochum',\n",
       " 'Herne',\n",
       " 'Thessaloniki',\n",
       " 'Nuremberg',\n",
       " 'Riga',\n",
       " 'Duisburg',\n",
       " 'Dortmund',\n",
       " 'Hanover',\n",
       " 'Zurich',\n",
       " 'Oslo',\n",
       " 'Bremen',\n",
       " 'Dresden',\n",
       " 'Sheffield',\n",
       " 'Palermo',\n",
       " 'Poznan',\n",
       " 'Gelsenkirchen',\n",
       " 'Bottrop',\n",
       " 'Bordeaux',\n",
       " 'Wroclaw',\n",
       " 'Gothenburg',\n",
       " 'Zaragoza',\n",
       " 'Genoa',\n",
       " 'Catania',\n",
       " 'The Hague',\n",
       " 'Toulouse',\n",
       " 'Bristol',\n",
       " 'Vilnius',\n",
       " 'Saarbrucken',\n",
       " 'Malaga',\n",
       " 'Nantes',\n",
       " 'Leeds',\n",
       " 'Nottingham',\n",
       " 'Florence',\n",
       " 'Gdansk',\n",
       " 'Leipzig',\n",
       " 'Mannheim',\n",
       " 'Belfast',\n",
       " 'Portsmouth',\n",
       " 'Venice',\n",
       " 'Edinburgh',\n",
       " 'Murcia',\n",
       " 'Nice',\n",
       " 'Liege',\n",
       " 'Bratislava',\n",
       " 'Leicester',\n",
       " 'Karlsruhe',\n",
       " 'Bergamo',\n",
       " 'Palma de Mallorca',\n",
       " 'Bologna',\n",
       " 'Bielefeld',\n",
       " 'Rouen',\n",
       " 'Strasbourg',\n",
       " 'Tallinn',\n",
       " 'Szczecin',\n",
       " 'Grenoble',\n",
       " 'Bari',\n",
       " 'Toulon',\n",
       " 'Brighton',\n",
       " 'Darmstadt',\n",
       " 'Wuppertal',\n",
       " 'Utrecht',\n",
       " 'Bournemouth',\n",
       " 'Middlesbrough',\n",
       " 'Geneva',\n",
       " 'Bydgoszcz',\n",
       " 'Basel',\n",
       " 'Kaunas',\n",
       " 'Brno',\n",
       " 'Southampton',\n",
       " 'Lens',\n",
       " 'Augsburg',\n",
       " 'Padua',\n",
       " 'Ostrava',\n",
       " 'Las Palmas',\n",
       " 'Constanta',\n",
       " 'Castellamare di Stabia',\n",
       " 'Torre Annunziata',\n",
       " 'Stoke',\n",
       " 'Santa Cruz de Tenerife',\n",
       " 'Lublin',\n",
       " 'Cardiff',\n",
       " 'Iasi',\n",
       " 'Plovdiv',\n",
       " 'Bradford',\n",
       " 'Alicante',\n",
       " 'Cluj',\n",
       " 'Napoca',\n",
       " 'Granada',\n",
       " 'Timisoara',\n",
       " 'Brescia',\n",
       " 'Galati',\n",
       " 'Montpellier',\n",
       " 'Varna',\n",
       " 'Verona',\n",
       " 'Busto Arsizio',\n",
       " 'Valladolid',\n",
       " 'Eindhoven',\n",
       " 'Charleroi',\n",
       " 'Cordoba',\n",
       " 'A Coruna',\n",
       " 'Craiova',\n",
       " 'Caserta',\n",
       " 'Coventry',\n",
       " 'Brasov',\n",
       " 'Bonn',\n",
       " 'Valletta',\n",
       " 'Ghent',\n",
       " 'Gdynia']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cities_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b27ab5",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e39ab1",
   "metadata": {},
   "source": [
    "## Trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "16e413fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trial = split_dump(wikidump[:1000])\n",
    "# len(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d66f7494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trial_d = process_dump(trial, cities_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98dc3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(trial_d, columns = ['article_id', 'title', 'text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4350fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_in_corpus(cities_ls, df.text[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3f4691",
   "metadata": {},
   "source": [
    "## Dump 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0f6a2d4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba74b16f94204ad282d752f47314703e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Progress:   0%|          | 0/8249 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# extract articles from first half of dump\n",
    "wikidump1 = split_dump(wikidump[:round(len(wikidump)/2)], cities_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc10ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikilength1 = len(wikidump1)\n",
    "wikilength1 = 5090764\n",
    "print(wikilength1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wikidump1 = process_dump(wikidump1, cities_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ca6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wikilength1 = len(p_wikidump1)\n",
    "# p_wikilength1 = 'nr that should actually go here'\n",
    "print(p_wikilength1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc9ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to .csv\n",
    "dumps = [p_wikidump1, p_wikidump2]\n",
    "i = 0\n",
    "df = pd.DataFrame(dumps[i], columns = ['article_id', 'title', 'text'])\n",
    "outputfp = f'../../../data/enwikidump{i+1}.csv'\n",
    "df.to_csv(outputfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ac1a2",
   "metadata": {},
   "source": [
    "## Dump2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f654b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract articles from second half of dump\n",
    "\n",
    "wikidump2 = split_dump(wikidump, cities_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50b46de",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikilength2 = len(wikidump2)\n",
    "# wikilength2 = nr that it should be\n",
    "\n",
    "print(wikilength2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28e3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "wikilength1 = 5090764 #len(wikidump1)\n",
    "wikilength2 = len(wikidump2)\n",
    "wikilength = wikilength1 + wikilength2\n",
    "\n",
    "print(f\"nr of articles in the enwiki dump is {wikilength}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28675ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wikidump2 = process_dump(wikidump2, cities_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03992c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wikilength2 = len(p_wikidump2)\n",
    "# p_wikilength2 = 'nr that should actually go here'\n",
    "print(p_wikilength2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef309506",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_wikilength = p_wikilength1 + p_wikilength2\n",
    "print(p_wikilength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e62f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to .csv\n",
    "dumps = [p_wikidump1, p_wikidump2]\n",
    "i = 1\n",
    "df = pd.DataFrame(dumps[i], columns = ['article_id', 'title', 'text'])\n",
    "outputfp = f'../../../data/enwikidump{i+1}.csv'\n",
    "df.to_csv(outputfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dcb77072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dumps = [p_wikidump1, p_wikidump2]\n",
    "# for i in range(2): \n",
    "#     df = pd.DataFrame(dumps[i], columns = ['article_id', 'title', 'text'])\n",
    "#     outputfp = f'../../../data/enwikidump{i+1}.csv'\n",
    "#     df.to_csv(outputfp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf347c58",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
