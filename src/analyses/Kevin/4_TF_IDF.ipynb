{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f2c9bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d7475",
   "metadata": {},
   "source": [
    "#### Load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "386c5d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv('tfidf_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd50ff4c",
   "metadata": {},
   "source": [
    "#### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79c5dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv('x_city_df.csv')\n",
    "df_0 = pd.read_csv('x_city_0_df.csv')\n",
    "city_names_df = pd.read_csv('city_variations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5d4fe1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = set(list(city_names_df['eng'])+list(city_names_df['wiki'])+list(city_names_df['local']))\n",
    "cities = cities.union(set([word.lower() for word in cities]))\n",
    "counter = 0\n",
    "\n",
    "total = len(df_1)\n",
    "results = {}\n",
    "for i, comment in enumerate(df_1['comment']):    \n",
    "    for j, city in enumerate(cities):\n",
    "        if city in comment:\n",
    "            counter +=1\n",
    "    results[i] = counter\n",
    "    counter = 0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "42eb342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_list = list(results.keys())\n",
    "val_list = list(results.values())\n",
    "scratch_df  = pd.DataFrame()\n",
    "scratch_df['key'] = key_list\n",
    "scratch_df['val'] = val_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d432e3",
   "metadata": {},
   "source": [
    "There is at least one city name in every comment from the untranslated x_city_1 data, but 600+ comments in the english version return no city name occurrance. We'll run the TF-IDF classifier on the comments first and then on the English and try to find a way to deal with the translation issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24656e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO DO: check if comments have been lemmatized. I think they came from unflattened unlemmatized folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ef0c48",
   "metadata": {},
   "source": [
    "#### Combine 0 and 1 responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "1febdc7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kodri\\AppData\\Local\\Temp\\ipykernel_24644\\544355394.py:5: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df_0.append(df_1)\n"
     ]
    }
   ],
   "source": [
    "y_0 = [0 for i in range(len(df_0))]\n",
    "y_1 = [1 for i in range(len(df_1))]\n",
    "df_0['Y'] = y_0\n",
    "df_1['Y'] = y_1\n",
    "df = df_0.append(df_1)\n",
    "cols = ['comment','language','english','Y']\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae20d46",
   "metadata": {},
   "source": [
    "#### Tokenization of 'english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6669a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4ea0163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(stem):\n",
    "\n",
    "    stem = stem.replace('.',' ')\n",
    "\n",
    "    data_into_list = stem.split(' ')\n",
    "    data_into_list = [value for value in data_into_list if value != '']\n",
    "    words = []\n",
    "    for word in data_into_list:\n",
    "    \n",
    "        #remove numbers\n",
    "        result = ''.join([j for j in word if not j.isdigit()])\n",
    "        #remove rest of punctuation\n",
    "        new_string = result.translate(str.maketrans('', '', string.punctuation))\n",
    "        if 'begindocument' in new_string:\n",
    "            new_string = new_string.replace('begindocument', '')\n",
    "        if new_string != '':\n",
    "            words.append(new_string)\n",
    "            \n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "49d1cef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_english = []\n",
    "for i in df['english']:\n",
    "    tokenized = tokenize(i)\n",
    "    tokenized_english.append(tokenized)\n",
    "    \n",
    "df['tokenized_english'] = tokenized_english"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddecc57b",
   "metadata": {},
   "source": [
    "#### Stop-word removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f63a935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(word_tokens):\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "\n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a95cc037",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stopwords = []\n",
    "for i in df['tokenized_english']:\n",
    "    no_stopwords.append(remove_stopwords(i))\n",
    "df['eng_tokenized_nosw'] = no_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddda181",
   "metadata": {},
   "source": [
    "#### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9f5d953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.porter import PorterStemmer\n",
    "porter_stemmer = PorterStemmer()\n",
    "df['stemmed_english'] = [[porter_stemmer.stem(word) for word in tokens] for tokens in df['eng_tokenized_nosw']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2347c940",
   "metadata": {},
   "source": [
    "#### Convert list back to string (for tf-idf function)¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3f42f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_to_string(df, col):\n",
    "    string = []\n",
    "    for i in df[col]:\n",
    "        comment = ''\n",
    "        for word in i:\n",
    "            comment = comment + word + ' '\n",
    "        string.append(comment)\n",
    "\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7e1fa7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save checkpoint\n",
    "df.to_csv('tfidf_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d22bf03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load checkpoint\n",
    "df = pd.read_csv('tfidf_df.csv')\n",
    "token_cols = ['tokenized_english', 'comment', 'eng_tokenized_nosw', 'stemmed_english']\n",
    "for i in token_cols:\n",
    "    df[i] = df[i].apply(lambda x: x.replace('[', '').replace(']', '').replace('\\'', '').split(', '))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b79e78",
   "metadata": {},
   "source": [
    "#### Default feature and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f5b4bad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_df = df[~df['eng_clean'].isna()].sample(frac=0.02, random_state=43)\n",
    "Y = samp_df['Y']\n",
    "#drop 'eng_clean' nan values that were \n",
    "corpus = [i for i in samp_df['eng_clean']]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "#X.todense() prints the vectorized tf-idf matrix\n",
    "\n",
    "tfidf_data = [X.toarray(), Y.to_numpy()]\n",
    "\n",
    "#train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_data[0], tfidf_data[1], test_size=0.33, random_state=72)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf79d51e",
   "metadata": {},
   "source": [
    "#### Naive Bayes (eng_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d162b86e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit model\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8d6267ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.90      0.61      0.72       631\n",
      "    Negative       0.71      0.93      0.81       647\n",
      "\n",
      "    accuracy                           0.77      1278\n",
      "   macro avg       0.80      0.77      0.76      1278\n",
      "weighted avg       0.80      0.77      0.76      1278\n",
      "\n",
      "Confusion matrix:\n",
      "[[382 249]\n",
      " [ 43 604]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=['Positive', 'Negative']))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999b872b",
   "metadata": {},
   "source": [
    "#### Naive Bayes (comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "40493e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment_clean'] = token_to_string(df, 'comment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3d3d7c40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp_df = df[~df['comment_clean'].isna()].sample(frac=0.02, random_state=3)\n",
    "Y = samp_df['Y']\n",
    "#drop 'eng_clean' nan values that were \n",
    "corpus = [i for i in samp_df['comment_clean']]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "#X.todense() prints the vectorized tf-idf matrix\n",
    "\n",
    "tfidf_data = [X.toarray(), Y.to_numpy()]\n",
    "\n",
    "#train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_data[0], tfidf_data[1], test_size=0.33, random_state=72)\n",
    "\n",
    "#fit model\n",
    "naive_bayes_classifier = MultinomialNB()\n",
    "naive_bayes_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7140dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.87      0.45      0.60       642\n",
      "    Negative       0.63      0.93      0.75       638\n",
      "\n",
      "    accuracy                           0.69      1280\n",
      "   macro avg       0.75      0.69      0.67      1280\n",
      "weighted avg       0.75      0.69      0.67      1280\n",
      "\n",
      "Confusion matrix:\n",
      "[[291 351]\n",
      " [ 44 594]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=['Positive', 'Negative']))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da427cba",
   "metadata": {},
   "source": [
    "#### Other classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6320affd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "61440665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_clf = LogisticRegression()\n",
    "logreg_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ee026f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positive       0.83      0.91      0.87       627\n",
      "    Negative       0.91      0.82      0.86       651\n",
      "\n",
      "    accuracy                           0.87      1278\n",
      "   macro avg       0.87      0.87      0.87      1278\n",
      "weighted avg       0.87      0.87      0.87      1278\n",
      "\n",
      "Confusion matrix:\n",
      "[[573  54]\n",
      " [114 537]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = logreg_clf.predict(X_test)\n",
    "print(metrics.classification_report(y_test, y_pred, target_names=['Positive', 'Negative']))\n",
    "print(\"Confusion matrix:\")\n",
    "print(metrics.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17134e8c",
   "metadata": {},
   "source": [
    "#### k-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75240df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b8085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Fold Cross-Validation\n",
    "def cross_validation(model, _X, _y, _cv=5):\n",
    "      '''Function to perform 5 Folds Cross-Validation\n",
    "       Parameters\n",
    "       ----------\n",
    "      model: Python Class, default=None\n",
    "              This is the machine learning algorithm to be used for training.\n",
    "      _X: array\n",
    "           This is the matrix of features.\n",
    "      _y: array\n",
    "           This is the target variable.\n",
    "      _cv: int, default=5\n",
    "          Determines the number of folds for cross-validation.\n",
    "       Returns\n",
    "       -------\n",
    "       The function returns a dictionary containing the metrics 'accuracy', 'precision',\n",
    "       'recall', 'f1' for both training set and validation set.\n",
    "      '''\n",
    "      _scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "      results = cross_validate(estimator=model,\n",
    "                               X=_X,\n",
    "                               y=_y,\n",
    "                               cv=_cv,\n",
    "                               scoring=_scoring,\n",
    "                               return_train_score=True)\n",
    "      \n",
    "      return {\"Training Accuracy scores\": results['train_accuracy'],\n",
    "              \"Mean Training Accuracy\": results['train_accuracy'].mean()*100,\n",
    "              \"Training Precision scores\": results['train_precision'],\n",
    "              \"Mean Training Precision\": results['train_precision'].mean(),\n",
    "              \"Training Recall scores\": results['train_recall'],\n",
    "              \"Mean Training Recall\": results['train_recall'].mean(),\n",
    "              \"Training F1 scores\": results['train_f1'],\n",
    "              \"Mean Training F1 Score\": results['train_f1'].mean(),\n",
    "              \"Validation Accuracy scores\": results['test_accuracy'],\n",
    "              \"Mean Validation Accuracy\": results['test_accuracy'].mean()*100,\n",
    "              \"Validation Precision scores\": results['test_precision'],\n",
    "              \"Mean Validation Precision\": results['test_precision'].mean(),\n",
    "              \"Validation Recall scores\": results['test_recall'],\n",
    "              \"Mean Validation Recall\": results['test_recall'].mean(),\n",
    "              \"Validation F1 scores\": results['test_f1'],\n",
    "              \"Mean Validation F1 Score\": results['test_f1'].mean()\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35e9fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped Bar Chart for both training and validation data\n",
    "def plot_result(x_label, y_label, plot_title, train_data, val_data):\n",
    "        '''Function to plot a grouped bar chart showing the training and validation\n",
    "          results of the ML model in each fold after applying K-fold cross-validation.\n",
    "         Parameters\n",
    "         ----------\n",
    "         x_label: str, \n",
    "            Name of the algorithm used for training e.g 'Decision Tree'\n",
    "          \n",
    "         y_label: str, \n",
    "            Name of metric being visualized e.g 'Accuracy'\n",
    "         plot_title: str, \n",
    "            This is the title of the plot e.g 'Accuracy Plot'\n",
    "         \n",
    "         train_result: list, array\n",
    "            This is the list containing either training precision, accuracy, or f1 score.\n",
    "        \n",
    "         val_result: list, array\n",
    "            This is the list containing either validation precision, accuracy, or f1 score.\n",
    "         Returns\n",
    "         -------\n",
    "         The function returns a Grouped Barchart showing the training and validation result\n",
    "         in each fold.\n",
    "        '''\n",
    "        \n",
    "        # Set size of plot\n",
    "        plt.figure(figsize=(12,6))\n",
    "        labels = [\"1st Fold\", \"2nd Fold\", \"3rd Fold\", \"4th Fold\", \"5th Fold\"]\n",
    "        X_axis = np.arange(len(labels))\n",
    "        ax = plt.gca()\n",
    "        plt.ylim(0.40000, 1)\n",
    "        plt.bar(X_axis-0.2, train_data, 0.4, color='blue', label='Training')\n",
    "        plt.bar(X_axis+0.2, val_data, 0.4, color='red', label='Validation')\n",
    "        plt.title(plot_title, fontsize=30)\n",
    "        plt.xticks(X_axis, labels)\n",
    "        plt.xlabel(x_label, fontsize=14)\n",
    "        plt.ylabel(y_label, fontsize=14)\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd49f57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NB_result = cross_validation(MultinomialNB(), X, Y)\n",
    "NB_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6940b97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot Accuracy Result\n",
    "model_name = \"NB\"\n",
    "plot_result(model_name,\n",
    "            \"Accuracy\",\n",
    "            \"Accuracy scores in 5 Folds\",\n",
    "            NB_result[\"Training Accuracy scores\"],\n",
    "            NB_result[\"Validation Accuracy scores\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecd87f9",
   "metadata": {},
   "source": [
    "#### Iterate through random states and validate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16000848",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    samp_df = df[~df['eng_clean'].isna()].sample(frac=0.10, random_state=i)\n",
    "    Y = samp_df['Y']\n",
    "    corpus = [i for i in samp_df['eng_clean']]\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    #train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(np.asarray(X.todense()), Y, test_size=0.33, random_state=72)\n",
    "    #fit model\n",
    "    naive_bayes_classifier = MultinomialNB()\n",
    "    naive_bayes_classifier.fit(X_train, y_train)\n",
    "    y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "    print(metrics.classification_report(y_test, y_pred, target_names=['Positive', 'Negative']))\n",
    "\n",
    "    NB_result = cross_validation(MultinomialNB(), X, Y)\n",
    "    print(NB_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9ae86e",
   "metadata": {},
   "source": [
    "#### Convert tfidf x into two-dimensions for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda4974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for performance metrics\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# Define dictionary with performance metrics\n",
    "scoring = {'accuracy':make_scorer(accuracy_score), \n",
    "           'precision':make_scorer(precision_score),\n",
    "           'recall':make_scorer(recall_score), \n",
    "           'f1_score':make_scorer(f1_score)}\n",
    "\n",
    "# Import required libraries for machine learning classifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Instantiate the machine learning classifiers\n",
    "log_model = LogisticRegression(max_iter=10000)\n",
    "svc_model = LinearSVC(dual=False)\n",
    "dtr_model = DecisionTreeClassifier()\n",
    "rfc_model = RandomForestClassifier()\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# Define the models evaluation function\n",
    "def models_evaluation(X, y, folds):\n",
    "    \n",
    "    '''\n",
    "    X : data set features\n",
    "    y : data set target\n",
    "    folds : number of cross-validation folds\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # Perform cross-validation to each machine learning classifier\n",
    "    log = cross_validate(log_model, X, y, cv=folds, scoring=scoring)\n",
    "    svc = cross_validate(svc_model, X, y, cv=folds, scoring=scoring)\n",
    "    dtr = cross_validate(dtr_model, X, y, cv=folds, scoring=scoring)\n",
    "    rfc = cross_validate(rfc_model, X, y, cv=folds, scoring=scoring)\n",
    "    gnb = cross_validate(gnb_model, X, y, cv=folds, scoring=scoring)\n",
    "\n",
    "    # Create a data frame with the models perfoamnce metrics scores\n",
    "    models_scores_table = pd.DataFrame({'Logistic Regression':[log['test_accuracy'].mean(),\n",
    "                                                               log['test_precision'].mean(),\n",
    "                                                               log['test_recall'].mean(),\n",
    "                                                               log['test_f1_score'].mean()],\n",
    "                                       \n",
    "                                      'Support Vector Classifier':[svc['test_accuracy'].mean(),\n",
    "                                                                   svc['test_precision'].mean(),\n",
    "                                                                   svc['test_recall'].mean(),\n",
    "                                                                   svc['test_f1_score'].mean()],\n",
    "                                       \n",
    "                                      'Decision Tree':[dtr['test_accuracy'].mean(),\n",
    "                                                       dtr['test_precision'].mean(),\n",
    "                                                       dtr['test_recall'].mean(),\n",
    "                                                       dtr['test_f1_score'].mean()],\n",
    "                                       \n",
    "                                      'Random Forest':[rfc['test_accuracy'].mean(),\n",
    "                                                       rfc['test_precision'].mean(),\n",
    "                                                       rfc['test_recall'].mean(),\n",
    "                                                       rfc['test_f1_score'].mean()],\n",
    "                                       \n",
    "                                      'Gaussian Naive Bayes':[gnb['test_accuracy'].mean(),\n",
    "                                                              gnb['test_precision'].mean(),\n",
    "                                                              gnb['test_recall'].mean(),\n",
    "                                                              gnb['test_f1_score'].mean()]},\n",
    "                                      \n",
    "                                      index=['Accuracy', 'Precision', 'Recall', 'F1 Score'])\n",
    "    \n",
    "    # Add 'Best Score' column\n",
    "    models_scores_table['Best Score'] = models_scores_table.idxmax(axis=1)\n",
    "    \n",
    "    # Return models performance metrics scores data frame\n",
    "    return(models_scores_table)\n",
    "  \n",
    "# Run models_evaluation function\n",
    "models_evaluation(X.toarray(), Y, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "33fa4470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.09301522, -0.09411812],\n",
       "        [-0.09629439, -0.09509063],\n",
       "        [-0.09395881, -0.09234162],\n",
       "        ...,\n",
       "        [-0.09758273, -0.02738785],\n",
       "        [-0.09606031,  0.00726774],\n",
       "        [-0.09119552, -0.08364914]]),\n",
       " array([0, 0, 0, ..., 1, 0, 0], dtype=int64)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
