{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "4f25446c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "import time \n",
    "import os\n",
    "\n",
    "#decompressing\n",
    "import bz2\n",
    "import lzma\n",
    "import zstandard as zstd\n",
    "\n",
    "#NLP\n",
    "from tqdm import tqdm_notebook\n",
    "import spacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "41941317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2017-07', '2020-12', '2020-01', '2021-1', 'RC_2020-11', '2020-04']"
      ]
     },
     "execution_count": 627,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#files to still download\n",
    "to_download = ['2017-07', '2020-12','2020-01', '2021-1'] #missing entirely from data\n",
    "too_small = ['RC_2020-11', '2020-04'] #both too small\n",
    "files = to_download + too_small\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "a7eda061",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_df = pd.read_csv('city_variations.csv')\n",
    "\n",
    "#filter to top 50. This should fix the 'Nice' occurrance issue\n",
    "#add lowercase to set\n",
    "top50_df = city_df.head(50)\n",
    "\n",
    "cities = set(list(top50_df['eng'])+list(top50_df['wiki'])+list(top50_df['local']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "043a989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kodri\\AppData\\Local\\Temp\\ipykernel_2620\\1199460390.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  top50_df['synonyms'] = synonyms\n"
     ]
    }
   ],
   "source": [
    "#find synonyms\n",
    "synonyms = []\n",
    "for i, row in top50_df.iterrows():\n",
    "    names = set([row['local'], row['eng'], row['wiki']])\n",
    "    if len(names) > 1:\n",
    "        synonyms.append(names)\n",
    "    else:\n",
    "        synonyms.append(np.nan)\n",
    "top50_df['synonyms'] = synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca645ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'citynet occurrances'\n",
    "files = os.listdir('citynet occurrances')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "f87b4a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [ 'created_utc', 'score', 'subreddit', 'link_id', 'subreddit_id', 'body']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b743d22a",
   "metadata": {},
   "source": [
    "### Create Co-Occurrance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64dc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For every month in the corpus, create a df of cooccurrances within comments. This should be a manageable size. \n",
    "# instad of copying all metadata, keep a record of index to go back and retrieve. \n",
    "# Later on group by link_id to find cooccurrances within comment threads\n",
    "#make a point to replace synonyms in cooccurrances so that Koln and Cologne don't appear, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "237809b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% files complete\n",
      "0% files complete\n",
      "1% files complete\n",
      "1% files complete\n",
      "2% files complete\n",
      "2% files complete\n",
      "3% files complete\n",
      "3% files complete\n",
      "4% files complete\n",
      "5% files complete\n",
      "5% files complete\n",
      "6% files complete\n",
      "6% files complete\n",
      "7% files complete\n",
      "7% files complete\n",
      "8% files complete\n",
      "8% files complete\n",
      "9% files complete\n",
      "10% files complete\n",
      "10% files complete\n",
      "11% files complete\n",
      "11% files complete\n",
      "12% files complete\n",
      "12% files complete\n",
      "13% files complete\n",
      "14% files complete\n",
      "14% files complete\n",
      "15% files complete\n",
      "15% files complete\n",
      "16% files complete\n",
      "16% files complete\n",
      "17% files complete\n",
      "17% files complete\n",
      "18% files complete\n",
      "19% files complete\n",
      "19% files complete\n",
      "20% files complete\n",
      "20% files complete\n",
      "21% files complete\n",
      "21% files complete\n",
      "22% files complete\n",
      "23% files complete\n",
      "23% files complete\n",
      "24% files complete\n",
      "24% files complete\n",
      "25% files complete\n",
      "25% files complete\n",
      "26% files complete\n",
      "26% files complete\n",
      "27% files complete\n",
      "28% files complete\n",
      "28% files complete\n",
      "29% files complete\n",
      "29% files complete\n",
      "30% files complete\n",
      "30% files complete\n",
      "31% files complete\n",
      "32% files complete\n",
      "32% files complete\n",
      "33% files complete\n",
      "33% files complete\n",
      "34% files complete\n",
      "34% files complete\n",
      "35% files complete\n",
      "35% files complete\n",
      "36% files complete\n",
      "37% files complete\n",
      "37% files complete\n",
      "38% files complete\n",
      "38% files complete\n",
      "39% files complete\n",
      "39% files complete\n",
      "40% files complete\n",
      "41% files complete\n",
      "41% files complete\n",
      "42% files complete\n",
      "42% files complete\n",
      "43% files complete\n",
      "43% files complete\n",
      "44% files complete\n",
      "44% files complete\n",
      "45% files complete\n",
      "46% files complete\n",
      "46% files complete\n",
      "47% files complete\n",
      "47% files complete\n",
      "48% files complete\n",
      "48% files complete\n",
      "49% files complete\n",
      "50% files complete\n",
      "50% files complete\n",
      "51% files complete\n",
      "51% files complete\n",
      "52% files complete\n",
      "52% files complete\n",
      "53% files complete\n",
      "53% files complete\n",
      "54% files complete\n",
      "55% files complete\n",
      "55% files complete\n",
      "56% files complete\n",
      "56% files complete\n",
      "57% files complete\n",
      "57% files complete\n",
      "58% files complete\n",
      "58% files complete\n",
      "59% files complete\n",
      "60% files complete\n",
      "60% files complete\n",
      "61% files complete\n",
      "61% files complete\n",
      "62% files complete\n",
      "62% files complete\n",
      "63% files complete\n",
      "64% files complete\n",
      "64% files complete\n",
      "65% files complete\n",
      "65% files complete\n",
      "66% files complete\n",
      "66% files complete\n",
      "67% files complete\n",
      "67% files complete\n",
      "68% files complete\n",
      "69% files complete\n",
      "69% files complete\n",
      "70% files complete\n",
      "70% files complete\n",
      "71% files complete\n",
      "71% files complete\n",
      "72% files complete\n",
      "73% files complete\n",
      "73% files complete\n",
      "74% files complete\n",
      "74% files complete\n",
      "75% files complete\n",
      "75% files complete\n",
      "76% files complete\n",
      "76% files complete\n",
      "77% files complete\n",
      "78% files complete\n",
      "78% files complete\n",
      "79% files complete\n",
      "79% files complete\n",
      "80% files complete\n",
      "80% files complete\n",
      "81% files complete\n",
      "82% files complete\n",
      "82% files complete\n",
      "83% files complete\n",
      "83% files complete\n",
      "84% files complete\n",
      "84% files complete\n",
      "85% files complete\n",
      "85% files complete\n",
      "86% files complete\n",
      "87% files complete\n",
      "87% files complete\n",
      "88% files complete\n",
      "88% files complete\n",
      "89% files complete\n",
      "89% files complete\n",
      "90% files complete\n",
      "91% files complete\n",
      "91% files complete\n",
      "92% files complete\n",
      "92% files complete\n",
      "93% files complete\n",
      "93% files complete\n",
      "94% files complete\n",
      "94% files complete\n",
      "95% files complete\n",
      "96% files complete\n",
      "96% files complete\n",
      "97% files complete\n",
      "97% files complete\n",
      "98% files complete\n",
      "98% files complete\n",
      "99% files complete\n"
     ]
    }
   ],
   "source": [
    "comment_cooc = {}\n",
    "path = 'citynet occurrances'\n",
    "files = os.listdir('citynet occurrances')\n",
    "\n",
    "total = len(files)\n",
    "counter = 0 \n",
    "\n",
    "for index, file in enumerate(files):\n",
    "    counter += 1\n",
    "    fp = os.path.join(path, file)\n",
    "    occurrances = {}\n",
    "    \n",
    "    with open(fp, 'r', encoding = 'utf-8') as f:\n",
    "        comments = f.read().split('\\n--------\\n')[:-1] #the last comment ofter the split is an empty string\n",
    "        \n",
    "        for idx, comment in enumerate(comments):\n",
    "            city_list = []\n",
    "            comment = comment.lower()\n",
    "            counter +=1\n",
    "            parts = comment.split(',')\n",
    "            body = parts[-2]\n",
    "            #split the comment body and search for toponym occurrances\n",
    "            words = body.split(' ')\n",
    "            for city in cities:\n",
    "                if city.lower() in words:\n",
    "                    city_list.append(city)\n",
    "            if len(city_list)>1:            \n",
    "                occurrances[idx] = city_list    \n",
    "    comment_cooc[fp] = occurrances\n",
    "            \n",
    "    print(str(100*index//total) + '% files complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "0b85a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create column labels for frequency matrix\n",
    "columns = ['doc', 'line']\n",
    "cities = list(cities)\n",
    "cities.sort()\n",
    "cols = columns + cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "6e053925",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create dummy variables and cooccurrance matrix\n",
    "cooc_matrix = []\n",
    "for i in comment_cooc.keys():\n",
    "    doc = i[-14:-4]\n",
    "    dummies = []\n",
    "    for line in comment_cooc[i]:\n",
    "        vector = [doc]\n",
    "        vector.append(line)\n",
    "        tops = comment_cooc[i][line]\n",
    "        for city in cities:\n",
    "            if city in tops:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        dummies.append(vector)\n",
    "    cooc_matrix = cooc_matrix + dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "cbef604f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_df = pd.DataFrame(cooc_matrix, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "756db04a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5% complete\n",
      "11% complete\n",
      "17% complete\n",
      "23% complete\n",
      "29% complete\n",
      "35% complete\n",
      "41% complete\n",
      "47% complete\n",
      "52% complete\n",
      "58% complete\n",
      "64% complete\n",
      "70% complete\n",
      "76% complete\n",
      "82% complete\n",
      "88% complete\n",
      "94% complete\n",
      "100% complete\n"
     ]
    }
   ],
   "source": [
    "#merge count columns referring to the same city\n",
    "\n",
    "tot = len(synonyms)\n",
    "counter = 1\n",
    "for pair in synonyms:\n",
    "    synonym = list(pair)\n",
    "    df = cooc_df[synonym]\n",
    "    merged = []\n",
    "    for idx, row in df.iterrows():\n",
    "        merged.append(row[synonym[0]]+row[synonym[1]])\n",
    "    new_col = synonym[0]+'_merged'\n",
    "    cooc_df[new_col] = merged\n",
    "    print(str(100*counter//tot)+'% complete')\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "6c175ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#check that synonym columns sum to '_merged'\n",
    "for i in synonyms:\n",
    "    a = list(synonyms[0])\n",
    "    a.append(a[0]+'_merged')\n",
    "    print(cooc_df[a].sum()[0]+cooc_df[a].sum()[1] == cooc_df[a].sum()[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "0fe8f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove columns that were already merged \n",
    "drop = set()\n",
    "for i in synonyms:\n",
    "    drop = drop.union(i)\n",
    "\n",
    "cols = [i for i in df.columns if i not in drop]\n",
    "df = cooc_df[cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72acffc4",
   "metadata": {},
   "source": [
    "#### Save/load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "4e72ca77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cooc_matrix_checkpoint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "c146d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cooc_matrix_checkpoint.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295d25f2",
   "metadata": {},
   "source": [
    "### Add other metadata to cooccurrance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "43ec395a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 647,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "4ecfa769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "#create and save cooccurrance index\n",
    "cooc_index = {}\n",
    "for i in df['doc'].unique():\n",
    "    temp_df = df[df['doc']==i]\n",
    "    indices = list(temp_df['line'])\n",
    "    cooc_index[i] = indices\n",
    "\n",
    "#save cooc_index checkpoint to file\n",
    "with open(r'cooc_index.txt', 'w') as fp:\n",
    "    for item in cooc_index:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\" % item)\n",
    "        fp.write(\": %s\\n\" % cooc_index[item])\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "2fa61fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8% done\n",
      "18% done\n",
      "28% done\n",
      "36% done\n",
      "46% done\n",
      "55% done\n",
      "64% done\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\kodri\\\\Desktop\\\\CITYNET Europe\\\\citynet occurrances\\\\C_2018-11..txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [466]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m indices \u001b[38;5;241m=\u001b[39m cooc_index[month]\n\u001b[0;32m     13\u001b[0m fp \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, file)\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     16\u001b[0m     comments \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--------\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\kodri\\\\Desktop\\\\CITYNET Europe\\\\citynet occurrances\\\\C_2018-11..txt'"
     ]
    }
   ],
   "source": [
    "#create df with all metadata except for text body\n",
    "counter = 0\n",
    "path = r\"C:\\Users\\kodri\\Desktop\\CITYNET Europe\\citynet occurrances\"\n",
    "meta_df = pd.DataFrame(columns=features[:5])\n",
    "counter = 0\n",
    "tracker = 0\n",
    "total = len(cooc_matrix)\n",
    "\n",
    "for month in cooc_index:\n",
    "    counter+=1\n",
    "    file = month + '.txt'\n",
    "    indices = cooc_index[month]\n",
    "    fp = os.path.join(path, file)\n",
    "    \n",
    "    with open(fp, 'r', encoding = 'utf-8') as f:\n",
    "        comments = f.read().split('\\n--------\\n')\n",
    "        for idx in indices:\n",
    "            chunks = comments[idx].split(',')\n",
    "            meta_df. loc[counter]=chunks[:5]\n",
    "            counter+=1\n",
    "            tracker+=1\n",
    "    if tracker > 100000:\n",
    "        tracker = 0\n",
    "        print(str(100*counter//total) + '% done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "4a72b787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['created_utc', 'score', 'subreddit', 'link_id', 'subreddit_id', 'body']"
      ]
     },
     "execution_count": 633,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "fee90a72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No co-occurrances in RC_2005-12\n",
      "No co-occurrances in RC_2006-01\n",
      "8% done\n",
      "18% done\n",
      "28% done\n",
      "36% done\n",
      "46% done\n",
      "55% done\n",
      "64% done\n",
      "73% done\n",
      "83% done\n",
      "92% done\n"
     ]
    }
   ],
   "source": [
    "#create df with all metadata except for text body\n",
    "counter = 0\n",
    "path = r\"C:\\Users\\kodri\\Desktop\\CITYNET Europe\\citynet occurrances\"\n",
    "counter = 0\n",
    "tracker = 0\n",
    "total = len(cooc_matrix)\n",
    "\n",
    "with open ('metadata.csv', 'w', encoding = 'utf-8') as w: \n",
    "    w.write(str(features[:5]).replace('[','').replace(']','').replace('\\'','')+'\\n')\n",
    "    for file in files:\n",
    "        month = file[:10]\n",
    "        counter+=1\n",
    "        if month in list(cooc_index.keys()):\n",
    "            indices = cooc_index[month]\n",
    "            fp = os.path.join(path, file)\n",
    "\n",
    "            with open(fp, 'r', encoding = 'utf-8') as f:\n",
    "                comments = f.read().split('\\n--------\\n')\n",
    "\n",
    "                for idx in indices:\n",
    "                    chunks = comments[idx].split(',')[:5]\n",
    "                    w.write(str(chunks).replace('[','').replace(']','').replace('\\'','')+'\\n')\n",
    "                    counter+=1\n",
    "                    tracker+=1\n",
    "            if tracker > 100000:\n",
    "                tracker = 0\n",
    "                print(str(100*counter//total) + '% done')\n",
    "        else:\n",
    "            print('No co-occurrances in ' + month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "bb390f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kodri\\AppData\\Local\\Temp\\ipykernel_2620\\4283592229.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  meta_df = pd.read_csv(r\"C:\\Users\\kodri\\Desktop\\CITYNET Europe\\metadata.csv\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_utc</th>\n",
       "      <th>score</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>link_id</th>\n",
       "      <th>subreddit_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1139003115</td>\n",
       "      <td>8</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>t3_18gy</td>\n",
       "      <td>t5_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1139517217</td>\n",
       "      <td>4</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>t3_1dp6</td>\n",
       "      <td>t5_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1143810425</td>\n",
       "      <td>16</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>t3_3qiz</td>\n",
       "      <td>t5_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1144908432</td>\n",
       "      <td>1</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>t3_4b3w</td>\n",
       "      <td>t5_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1146177212</td>\n",
       "      <td>2</td>\n",
       "      <td>reddit.com</td>\n",
       "      <td>t3_5y81a</td>\n",
       "      <td>t5_6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   created_utc  score    subreddit    link_id  subreddit_id\n",
       "0   1139003115      8   reddit.com    t3_18gy          t5_6\n",
       "1   1139517217      4   reddit.com    t3_1dp6          t5_6\n",
       "2   1143810425     16   reddit.com    t3_3qiz          t5_6\n",
       "3   1144908432      1   reddit.com    t3_4b3w          t5_6\n",
       "4   1146177212      2   reddit.com   t3_5y81a          t5_6"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df = pd.read_csv(r\"C:\\Users\\kodri\\Desktop\\CITYNET Europe\\metadata.csv\")\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 637,
   "id": "085eed27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 637,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cooc_df) == len(meta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "3cf5a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.to_csv('meta_df_checkpoint.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "f2087706",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([cooc_df, meta_df.reindex(cooc_df.index)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "288f7c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove columns that were already merged \n",
    "drop = set()\n",
    "for i in synonyms:\n",
    "    drop = drop.union(i)\n",
    "\n",
    "cols = [i for i in df.columns if i not in drop]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "id": "cc763f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "id": "75f30734",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cooc_df.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
