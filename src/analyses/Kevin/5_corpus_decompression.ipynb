{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcdc46cb",
   "metadata": {},
   "source": [
    "## 1. Filter for Toponym Occurrances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8698ff27",
   "metadata": {},
   "source": [
    "1. Download a year of comments\n",
    "2. load country comments set\n",
    "3. decompress country comments\n",
    "4. find occurrances of set elements in year and check new file size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf14343",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import time \n",
    "import os\n",
    "\n",
    "#decompressing\n",
    "import bz2\n",
    "import lzma\n",
    "import zstandard as zstd\n",
    "\n",
    "#NLP\n",
    "from tqdm import tqdm_notebook\n",
    "import spacy\n",
    "from collections import Counter\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc4addb",
   "metadata": {},
   "source": [
    "#### Create cities list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf64560",
   "metadata": {},
   "source": [
    "### Get one document at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2b76f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "city_df = pd.read_csv('city_variations.csv')\n",
    "\n",
    "#filter to top 50. This should fix the 'Nice' occurrance issue\n",
    "#add lowercase to set\n",
    "top50_df = city_df.head(50)\n",
    "\n",
    "cities = set(list(top50_df['eng'])+list(top50_df['wiki'])+list(top50_df['local']))\n",
    "\n",
    "low = []\n",
    "\n",
    "for i in cities:\n",
    "    low.append(i.lower())\n",
    "\n",
    "cities = cities | set(low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75332630",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_filter = [ 'created_utc', 'score', 'subreddit',\n",
    "        'link_id', 'subreddit_id', 'body']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa71a90f",
   "metadata": {},
   "source": [
    "#### bz2 to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "648c5e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = [\n",
    " 'RC_2015-09.bz2',\n",
    " 'RC_2015-10.bz2',\n",
    " 'RC_2015-11.bz2',\n",
    " 'RC_2015-12.bz2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "9ebcef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bz2_to_df_1(file, open_fp, write_fp, cities):\n",
    "    counter = 0\n",
    "    with bz2.open(open_fp+file, \"rt\") as bzinput:\n",
    "        lines = []\n",
    "        with open(write_fp+file[:-4]+'.txt', \"w\", encoding='utf-8') as f:\n",
    "            for i, line in enumerate(bzinput):\n",
    "                post = json.loads(line)\n",
    "                for top in cities:\n",
    "                    #filter before tokenizing\n",
    "                    if top in post['body']:\n",
    "                        #tokenize\n",
    "                        words = post['body'].split()\n",
    "                        if top in words:\n",
    "                            #idx = int(post['body'].find(top))\n",
    "                            #upper = idx+len(top)+3\n",
    "                            #lower = idx - 3\n",
    "                            #neighborhood = post['body'][lower:upper]\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            for i in col_filter:\n",
    "                                f.write(str(post[i]).replace(',' , '').replace('\\n', ' ') + ',')\n",
    "                                \n",
    "                            f.write('\\n--------\\n')\n",
    "                            break\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "46768998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3775.986893892288 seconds\n",
      "3256.664025783539 seconds\n",
      "3138.3325169086456 seconds\n",
      "3204.11865735054 seconds\n"
     ]
    }
   ],
   "source": [
    "open_fp = r\"H:\\\\data\\\\\"\n",
    "\n",
    "for file in files:\n",
    "\n",
    "    start = time.time()\n",
    "    write_fp = r\"C:\\\\Users\\\\kodri\\\\Desktop\\\\CITYNET Europe\\\\citynet occurrances\\\\\"\n",
    "    bz2_to_df_1(file, open_fp, write_fp, cities)\n",
    "\n",
    "    end = time.time() \n",
    "    print(str(end-start) + ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c45a3c",
   "metadata": {},
   "source": [
    "#### lzma to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "fb490850",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['RC_2018-04.xz','RC_2018-05.xz','RC_2018-06.xz', 'RC_2018-07.xz']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "be8729d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lz_to_df_1(file, open_fp, write_fp, cities):\n",
    "    counter = 0\n",
    "    with lzma.open(open_fp+file, \"rt\") as lzinput:\n",
    "        lines = []\n",
    "        with open(write_fp+file[:-3]+'.txt', \"w\", encoding='utf-8') as f:\n",
    "            for i, line in enumerate(lzinput):\n",
    "                post = json.loads(line)\n",
    "                for top in cities:\n",
    "                    #filter before tokenizing\n",
    "                    if top in post['body']:\n",
    "                        #tokenize\n",
    "                        words = post['body'].split()\n",
    "                        if top in words:\n",
    "                            #idx = int(post['body'].find(top))\n",
    "                            #upper = idx+len(top)+3\n",
    "                            #lower = idx - 3\n",
    "                            #neighborhood = post['body'][lower:upper]\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            for i in col_filter:\n",
    "                                f.write(str(post[i]).replace(',' , '').replace('\\n', ' ') + ',')\n",
    "                                \n",
    "                            f.write('\\n--------\\n')\n",
    "                            break\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "739642cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6584.823339223862 seconds\n",
      "4865.527549266815 seconds\n",
      "5339.1321902275085 seconds\n",
      "5911.253631591797 seconds\n"
     ]
    }
   ],
   "source": [
    "for file in files:\n",
    "\n",
    "    start = time.time()\n",
    "    write_fp = r\"E:\\\\citynet cooccurrances\\\\\"\n",
    "    lz_to_df_1(file, open_fp, write_fp, cities)\n",
    "\n",
    "    end = time.time() \n",
    "    print(str(end-start)+' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd2b0c6",
   "metadata": {},
   "source": [
    "#### zstd to .txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e10feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff78819",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zst_to_df_1(file, open_fp, write_fp, cities):\n",
    "    counter = 0\n",
    "    with zst.open(open_fp+file, \"rt\") as zstinput:\n",
    "        lines = []\n",
    "        with open(write_fp+file[:-3]+'.txt', \"w\", encoding='utf-8') as f:\n",
    "            for i, line in enumerate(zstinput):\n",
    "                post = json.loads(line)\n",
    "                for top in cities:\n",
    "                    #filter before tokenizing\n",
    "                    if top in post['body']:\n",
    "                        #tokenize\n",
    "                        words = post['body'].split()\n",
    "                        if top in words:\n",
    "                            #idx = int(post['body'].find(top))\n",
    "                            #upper = idx+len(top)+3\n",
    "                            #lower = idx - 3\n",
    "                            #neighborhood = post['body'][lower:upper]\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            for i in col_filter:\n",
    "                                f.write(str(post[i]).replace(',' , '').replace('\\n', ' ') + ',')\n",
    "                                \n",
    "                            f.write('\\n--------\\n')\n",
    "                            break\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d880b3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "\n",
    "    start = time.time()\n",
    "    write_fp = r\"E:\\\\citynet cooccurrances\\\\\"\n",
    "    zst_to_df_1(file, open_fp, write_fp, cities)\n",
    "\n",
    "    end = time.time() \n",
    "    print(str(end-start)+' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c97f609",
   "metadata": {},
   "source": [
    "#### zst decompression streaming for files too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "fc51d76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806cb63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    with open(open_fp+file, 'rb') as fh:\n",
    "        dctx = zstd.ZstdDecompressor(max_window_size=2147483648)\n",
    "        with dctx.stream_reader(fh) as reader:\n",
    "            previous_line = \"\"\n",
    "\n",
    "            with open(write_fp+file[:-4]+'.txt', \"w\", encoding='utf-8') as f:\n",
    "\n",
    "                while True:\n",
    "                    chunk = reader.read(2**24)  # 16mb chunks\n",
    "                    if not chunk:\n",
    "                        break\n",
    "\n",
    "                    string_data = chunk.decode('utf-8')\n",
    "                    lines = string_data.split(\"\\n\")\n",
    "                    for i, line in enumerate(lines[:-1]):\n",
    "                        if i == 0:\n",
    "                            line = previous_line + line\n",
    "                        try:\n",
    "                            post = json.loads(line)\n",
    "                        except:\n",
    "                            continue\n",
    "\n",
    "                        for top in cities:\n",
    "                            #filter before tokenizing\n",
    "                            if top in post['body']:\n",
    "                                #tokenize\n",
    "                                words = post['body'].split()\n",
    "                                if top in words:\n",
    "                                    #idx = int(post['body'].find(top))\n",
    "                                    #upper = idx+len(top)+3\n",
    "                                    #lower = idx - 3\n",
    "                                    #neighborhood = post['body'][lower:upper]\n",
    "\n",
    "\n",
    "\n",
    "                                    for i in col_filter:\n",
    "                                        f.write(str(post[i]).replace(',' , '').replace('\\n', ' ') + ',')\n",
    "\n",
    "                                    f.write('\\n--------\\n')\n",
    "                                    break\n",
    "\n",
    "                previous_line = lines[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453b3c6c",
   "metadata": {},
   "source": [
    "#### Speed Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9604dc",
   "metadata": {},
   "source": [
    "-loop through upper and lower \n",
    "\n",
    "-convert post to lower and only loop lower cities\n",
    "\n",
    "-find cooccurrances as well as occurrances\n",
    "\n",
    "-decision: loop through upper and lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037628be",
   "metadata": {},
   "source": [
    "## 2. Co-occurrances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60def82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_download = ['2017-07','2020-12','2020-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "923c5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_lc = set([city.lower() for city in cities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6a7ea4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kodri\\\\Desktop\\\\CITYNET Europe\\\\citynet occurrances\\\\RC_2005-12.txt'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r\"C:\\Users\\kodri\\Desktop\\CITYNET Europe\\citynet occurrances\"\n",
    "files = os.listdir(fp)\n",
    "os.path.join(fp, files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ba311936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1135307868,0,reddit.com,t3_20714,t5_6,American here in UK circa 1992 and 2001.  On the last trip there the proliferation of CCTV cameras on streets and buildings in London and outlying areas was very noticeable.\n",
      "\n",
      " \n",
      "\n",
      " Any thoughts on how past socialism movements in the UK weakened the UK to the point where abuse of this scale is now acceptable?\n",
      "\n",
      " \n",
      "\n",
      " That could be what a previous commentor meant when he referred to \"hello Nazi UK.\",\n",
      "\n",
      "--------\n",
      "\n",
      "1135407792,2,reddit.com,t3_21213,t5_6,East London for a long while was fairly run down cheaper [property] part of London. Recently it has slowly changed to a much safer part of town that many younger affluent professionals look to move into for the nightlife and social benefits that living in a lively part of the city provides. Iâm guessing it wonât be long before they will be an exodus of âcreativeâ types from East London also.,\n",
      "\n",
      "--------\n",
      "\n",
      "1135830178,1,reddit.com,t3_22099,t5_6,Spoof posters depicting Britain's Queen Elizabeth having sex with the U.S. and French presidents that are displayed across Vienna are causing embarrassment just days ahead of Austria's taking over the EU presidency.\n",
      "\n",
      " \n",
      "\n",
      "     Part of a series called \"euroPART\" that features art created by artists from all 25 member countries of the European Union the posters were meant to \"reflect on the different social historical and political developments in Europe\" said art project 25peaces which commissioned the posters.,\n",
      "\n",
      "--------\n",
      "\n",
      "1136881391,6,reddit.com,t3_25099,t5_6,The article is great. Richard Dawkins is doing amazing stuff at the moment. He's launching a one man crusade against religion. If anyone missed the uk channel 4 broadcast of his program \"Root of all evil?\" last night track it down. There are some incredible things in there. Particularly striking is when he meets an evangelist and compares his sermon to the nuremberg rallies.,\n",
      "\n",
      "--------\n",
      "\n",
      "1137092920,1,reddit.com,t3_26024,t5_6,Even Paris Hilton glows green with the right kind of camera.,\n",
      "\n",
      "--------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_cooccurrances(path, files, cities):\n",
    "    cities = set([city.lower() for city in cities])\n",
    "    output = []\n",
    "    counter = 0\n",
    "    stahp = 0\n",
    "    for file in files:\n",
    "        fp = os.path.join(path, file)\n",
    "        cooc = []\n",
    "        stahp+=1\n",
    "        if stahp<3:\n",
    "        \n",
    "            with open(fp, encoding='utf-8') as f:\n",
    "                text = ''\n",
    "                for i, line in enumerate(f):\n",
    "                    if line != '--------':\n",
    "                        text = text + ' ' + line\n",
    "                    else:\n",
    "                        \n",
    "\n",
    "    return output\n",
    "\n",
    "test = get_cooccurrances(fp, files, cities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
