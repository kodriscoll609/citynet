{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ce19bbb",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "3fba8717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_functions import *\n",
    "from general_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "1179dc49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner', 'parser']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.disable_pipes('ner', 'parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ef62c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_path(path, OVERRIDE=False):\n",
    "#     \"\"\"\n",
    "#     -->\n",
    "#         function that checks if file exists and if so, asks the user if they want to override it.\n",
    "\n",
    "#         Parameters:\n",
    "#             -----------\n",
    "#                 path: str -> path to check\n",
    "\n",
    "#     \"\"\"\n",
    "#     if os.path.exists(path):\n",
    "#         if OVERRIDE:\n",
    "#             print(f\"Overriding the following file: {path}\")\n",
    "#             return True\n",
    "#         else:\n",
    "#             decision = input(f\"This following path already exists:\\n '{path}'\\nAre you sure you want to override?\\n Continue? [y/n]: \")\n",
    "            \n",
    "#             if decision == 'y':\n",
    "#                 return True\n",
    "#                 print(\"The process has been continued.\")\n",
    "#             elif decision == 'n':\n",
    "#                 print(\"The process has been halted.\")\n",
    "#                 return False\n",
    "#             else:\n",
    "#                 print(\"You did not enter a valid option. \\nCanceling Operation...\")\n",
    "#                 return False\n",
    "#     else:\n",
    "#         return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3046144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatise_paragraphs(df, OUTPUT_PATH, POS, ONLY_ENGLISH_WORDS=False, ENGLISH_WORD_LIST=[], OVERWRITE=False, NLP_MAX_LENGTH=1500000):\n",
    "#     \"\"\"\n",
    "#     -->\n",
    "#         function that lemmatises the paragraphs of a single text file.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#             FILE_PATH: Str -> input directory path, to the text files\n",
    "#             FILE_OUTPUT_DIR: Str -> output directory path, where you want to save the .pickle files\n",
    "#             POS: string (e.g. \"NOUN\") -> options: (https://spacy.io/usage/spacy-101#annotations-pos-deps)\n",
    "#             OVERRIDE_OLD_WORDLISTS: Bool -> Whether you want to override existing output files\n",
    "#             NLP_MAX_LENGTH: Int (default: 1500000) -> Allowed number of characters per file\n",
    "#     \"\"\"\n",
    "    \n",
    "#     nlp.max_length = NLP_MAX_LENGTH\n",
    "    \n",
    "#     #Checks if valid part-of-speech tag was provided\n",
    "#     POStags=[\"PROPN\", \"AUX\", \"NOUN\", \"ADJ\", \"VERB\", \"ADP\", \"SYM\", \"NUM\"]\n",
    "#     if not isinstance(POS, str) or POS.upper() not in POStags:\n",
    "#         raise Exception(f'POSfilter only allows any of the following (SpaCy) part-of-speech tags: {POStags}.')\n",
    "    \n",
    "#     paragraphs_dict = {}\n",
    "#     if check_path(OUTPUT_PATH, OVERWRITE):\n",
    "#         processed_paragraphs = [text for text in tqdm(nlp.pipe(df.paragraph, n_process=2, batch_size=1, disable=[\"ner\", \"parser\"]), desc=f\"Lemmatising ({OUTPUT_PATH.split('___')[1]})\",total=len(df.paragraph), leave=False)]\n",
    "#         lemmatized_paragraphs = [[word.lemma_ for word in paragraph if word.pos_ == POS and not word.is_punct and not word.is_stop] for paragraph in processed_paragraphs]\n",
    "#         regexed_paragraphs= [[re.sub(r'\\W+', '', word) for word in paragraph] for paragraph in lemmatized_paragraphs]\n",
    "        \n",
    "#         for index, lemmatised_paragraph in enumerate(regexed_paragraphs):\n",
    "#             paragraphs_dict[df.loc[index].paragraph_id] = lemmatised_paragraph\n",
    "\n",
    "#         with open(OUTPUT_PATH, 'wb') as fp:\n",
    "#             pickle.dump(paragraphs_dict, fp)\n",
    "    \n",
    "#     filename = os.path.basename(OUTPUT_PATH)\n",
    "#     CLEAN_PATH = f\"{os.path.dirname(OUTPUT_PATH)}/{'_CLEAN.'.join(filename.split('.'))}\"\n",
    "\n",
    "#     if ONLY_ENGLISH_WORDS and check_path(CLEAN_PATH, OVERWRITE):\n",
    "#         if not paragraphs_dict:\n",
    "#             with open(OUTPUT_PATH, 'rb') as file_read:\n",
    "#                     paragraphs_dict = pickle.load(file_read)\n",
    "                    \n",
    "#         for paragraph_id in tqdm(paragraphs_dict, desc='Removing non-existent words', leave=False):\n",
    "#             cleaned_lemmatised_paragraph = remove_non_existing_words(paragraphs_dict[paragraph_id], ENGLISH_WORD_LIST)\n",
    "#             paragraphs_dict[paragraph_id] = cleaned_lemmatised_paragraph\n",
    "\n",
    "#         with open(CLEAN_PATH, 'wb') as file_write:\n",
    "#             pickle.dump(paragraphs_dict, file_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ceddc106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def is_english_word(word, english_words):\n",
    "#     return word.lower() in english_words\n",
    "\n",
    "# def remove_non_existing_words(wordlist: list, english_words) -> list:\n",
    "#     if not len(english_words):\n",
    "#         raise Exception(\"The supplied english words list is empty.\"\n",
    "#                        )\n",
    "#     wordset = set(wordlist)\n",
    "#     non_existent = []\n",
    "    \n",
    "#     for word in wordset:\n",
    "#         if not is_english_word(word, english_words):\n",
    "#             non_existent.append(word)\n",
    "            \n",
    "#     return([word for word in wordlist if word not in non_existent])            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3e0c44d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "430361e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_POS(POS):\n",
    "#     POStags=[\"PROPN\", \"AUX\", \"NOUN\", \"ADJ\", \"VERB\", \"ADP\", \"SYM\", \"NUM\"]\n",
    "    \n",
    "#     #Checks if valid part-of-speech tag was provided\n",
    "#     if isinstance(POS, str):\n",
    "#         if POS.upper() not in POStags:\n",
    "#             raise Exception(f'POSfilter only allows any of the following (SpaCy) part-of-speech tags: {POStags}.')\n",
    "#     elif isinstance(POS, list):\n",
    "#         for tag in POS:\n",
    "#             if tag.upper() not in POStags:\n",
    "#                 raise Exception(f'POSfilter only allows any of the following (SpaCy) part-of-speech tags: {POStags}.')\n",
    "    \n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "d88aa44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lemmatise(INPUT_DIR, POS, BATCHES=[], LEMMATISATION_TYPE='', ONLY_ENGLISH_WORDS=False, english_words_file=\"../../../input/english_words_alpha_370k.txt\", OVERWRITE=False):   \n",
    "#     BATCHES = [int(x) for x in BATCHES]\n",
    "#     reg_str = 'biggest_cities_([0-9]+)'\n",
    "    \n",
    "#     #Checks if valid part-of-speech tag was provided\n",
    "#     POStags = [\"PROPN\", \"AUX\", \"NOUN\", \"ADJ\", \"VERB\", \"ADP\", \"SYM\", \"NUM\"]\n",
    "#     if not isinstance(POS, list) or len([tag.upper() for tag in POS if tag not in POStags]):\n",
    "#         raise Exception(f'POSfilter only allows any of the following (SpaCy) part-of-speech tags: {POStags}.')\n",
    "    \n",
    "#     if ONLY_ENGLISH_WORDS:\n",
    "#         with open(english_words_file) as word_file:\n",
    "#             ENGLISH_WORDS = set(word.strip().lower() for word in word_file)\n",
    "\n",
    "#     batch_dirs = [os.path.join(INPUT_DIR, batch) for batch in os.listdir(INPUT_DIR) if not BATCHES or int(re.findall(reg_str, batch)[0]) in BATCHES]\n",
    "\n",
    "#     # Where the magic happens\n",
    "#     for batch_dir in tqdm(batch_dirs, desc=f\"BATCHES: {BATCHES}\"):\n",
    "        \n",
    "#         for citypair in tqdm(os.listdir(batch_dir), desc=\"City Pair\", leave=False):\n",
    "#             citypair_dir = os.path.join(batch_dir, citypair)\n",
    "#             CITY_PAIR = citypair.split('___')[1]\n",
    "\n",
    "#             df_paragraphs_path = f\"{citypair_dir}/{CITY_PAIR}.csv\"\n",
    "#             if os.path.exists(df_paragraphs_path):\n",
    "#                 df = pd.read_csv(df_paragraphs_path)\n",
    "\n",
    "#                 for tag in tqdm(POS, desc=f\"POS: {POS}\", leave=False):\n",
    "#                     POS_path = f\"{citypair_dir}/lemmatisation/{tag}.pickle\"\n",
    "#                     lemmatise_paragraphs(df=df, \n",
    "#                                          OUTPUT_PATH=POS_path,\n",
    "#                                          POS=tag,\n",
    "#                                          OVERWRITE=OVERWRITE,\n",
    "#                                          ONLY_ENGLISH_WORDS=ONLY_ENGLISH_WORDS,\n",
    "#                                          ENGLISH_WORD_LIST = ENGLISH_WORDS,\n",
    "#                                          NLP_MAX_LENGTH=1500000)\n",
    "\n",
    "\n",
    "\n",
    "                    \n",
    "#             for foldertype in ['classification, frequencies', 'lemmatisation']:\n",
    "#                 os.makedirs(f\"{citypair_dir}/{foldertype}\", exist_ok=True)\n",
    "\n",
    "\n",
    "#             if lemmatisation:\n",
    "#                 for tag in POS:\n",
    "\n",
    "#                     FILE_PATH = os.path.join(root, file)\n",
    "#                     FILE_OUTPUT_DIR = root + '/' + 'lemmatisation'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ea388707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c49d9f85384ba79f5eb57c9745be7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BATCHES: [5]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "City Pair:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (berlin_milan):   0%|          | 0/929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (berlin_milan):   0%|          | 0/929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (london_berlin):   0%|          | 0/7389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/7389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (london_berlin):   0%|          | 0/7389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/7389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (london_madrid):   0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (london_madrid):   0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (london_milan):   0%|          | 0/2512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/2512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (london_milan):   0%|          | 0/2512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/2512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (madrid_berlin):   0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (madrid_berlin):   0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (madrid_milan):   0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (madrid_milan):   0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_berlin):   0%|          | 0/7612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/7612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_berlin):   0%|          | 0/7612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/7612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_london):   0%|          | 0/21051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/21051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_london):   0%|          | 0/21051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/21051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_madrid):   0%|          | 0/3277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/3277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_madrid):   0%|          | 0/3277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/3277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_milan):   0%|          | 0/3251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/3251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_milan):   0%|          | 0/3251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/3251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 8s\n",
      "Wall time: 18min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "INPUT_DIR = \"../../../../../data/clean/city_pair_paragraphs/\"\n",
    "BATCHES = [5]\n",
    "POS = [\"VERB\", \"ADJ\"]\n",
    "# LEMMATISATION_TYPE = 'quick', 'accurate'\n",
    "ONLY_ENGLISH_WORDS = True\n",
    "OVERWRITE = False\n",
    "\n",
    "lemmatise(INPUT_DIR, POS, BATCHES, ONLY_ENGLISH_WORDS=ONLY_ENGLISH_WORDS, OVERWRITE=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ced218f",
   "metadata": {},
   "source": [
    "### Open single file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fadba8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "P = \"../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___paris_london___/lemmatisation/NOUN_CLEAN.pickle\"\n",
    "with open(P, 'rb') as file_read:\n",
    "    paragraphs = pickle.load(file_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "b802ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n",
      "57\n",
      "37\n",
      "9\n",
      "12\n",
      "22\n",
      "13\n",
      "16\n",
      "23\n",
      "28\n"
     ]
    }
   ],
   "source": [
    "for x in list(paragraphs.keys())[:10]:\n",
    "    print(len(paragraphs[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "69ad1512",
   "metadata": {},
   "outputs": [],
   "source": [
    "P2 = \"../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___paris_london___/lemmatisation/NOUN.pickle\"\n",
    "with open(P2, 'rb') as file_read:\n",
    "    paragraphs2 = pickle.load(file_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "1d4b29d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['screenwriter',\n",
       "  'alma',\n",
       "  'oratory',\n",
       "  'couple',\n",
       "  'floor',\n",
       "  'hour',\n",
       "  'insistence',\n",
       "  'mother',\n",
       "  'cathedral'],\n",
       " ['screenwriter',\n",
       "  'alma',\n",
       "  'reville',\n",
       "  'oratory',\n",
       "  'couple',\n",
       "  'floor',\n",
       "  'hour',\n",
       "  'insistence',\n",
       "  'mother',\n",
       "  'cathedral'])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[23056], paragraphs2[23056]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "b777acd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reville'}"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(paragraphs2[23056]).difference(set(paragraphs[23056]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bc4b9e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23053 []\n",
      "23054 []\n",
      "23055 ['1760']\n",
      "23056 ['reville']\n",
      "23057 []\n",
      "23058 ['grachtengordel']\n",
      "23059 []\n",
      "23060 ['spoorwegen', 'holendrecht', 'muiderpoort', 'spoorwegen', 'eurostar']\n",
      "23061 []\n",
      "23062 ['lyngstad', 'ulvaeus', 'lyngstad', 'lyngstad']\n",
      "23063 ['1860']\n",
      "23064 ['haggadot', 'noyon']\n",
      "23065 []\n",
      "23066 ['différent', 'système', 'lumière', 'lumière']\n",
      "23067 ['crowley', 'blythe', 'crowley']\n",
      "23068 ['ringler', 'neuburg', 'neuburg', 'neuburg', 'crowley', 'crowley']\n",
      "23069 ['ménage']\n",
      "23070 ['crowley']\n",
      "23072 []\n",
      "23073 ['solzhenitsyn', 'solzhenitsyn', 'solzhenitsyn']\n",
      "23074 []\n",
      "23075 ['déco', 'déco', '1930']\n",
      "23076 ['1930', 'grauman', 'façade', 'gaumont', 'impero']\n",
      "23077 ['30', 'mossehaus', 'chilehaus', 'anzeiger']\n",
      "23078 ['penzance']\n",
      "23079 []\n",
      "23080 []\n",
      "23081 []\n",
      "23082 []\n",
      "23083 []\n",
      "23084 []\n",
      "23085 []\n",
      "23086 ['a300']\n",
      "23087 ['fayum']\n",
      "23088 ['eyüp']\n",
      "23089 []\n",
      "23090 ['bahá']\n",
      "23091 []\n",
      "23092 ['ʻabdul', 'ʻabdul', 'bahá']\n",
      "23093 []\n",
      "23094 ['schauspielhaus', 'sächsische', 'kinderchor', 'rootering']\n",
      "23095 ['hsl', 'lille', 'hsl', 'liège', 'hsl', 'liège']\n",
      "23096 ['mid1998']\n",
      "23097 []\n",
      "23098 []\n",
      "23099 ['51min', 'thaly', '50min', '25min', 'thaly', '50min', '57min']\n",
      "23100 []\n",
      "23101 ['élysées']\n",
      "23102 ['aerospatiale']\n",
      "23103 []\n"
     ]
    }
   ],
   "source": [
    "for p_id in list(paragraphs2.keys())[:50]:\n",
    "    l = []\n",
    "    for word in paragraphs2[p_id]:\n",
    "        # print(word)\n",
    "        if word not in paragraphs[p_id]:\n",
    "            l.append(word)\n",
    "    print(p_id, l)\n",
    "    # print(len(paragraphs2[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da1ab8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
