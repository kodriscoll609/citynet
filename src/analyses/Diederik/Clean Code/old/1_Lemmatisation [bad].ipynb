{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c8f9a10",
   "metadata": {},
   "source": [
    "# Lemmatize paragraphs individually (NOUNS & VERBS & ADJECTIVES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b14a584b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_functions import *\n",
    "from general_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d9bb7c",
   "metadata": {},
   "source": [
    "### Folder Structure Creation (for each city_pair) function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b073aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIR = \"../../../../../data/clean/city_pair_paragraphs/biggest_cities_10\"\n",
    "\n",
    "for batch in os.listdir(INPUT_DIR):\n",
    "    batch_dir = os.path.join(INPUT_DIR, batch)\n",
    "    for citypair in os.listdir(batch_dir):\n",
    "        citypair_dir = os.path.join(batch_dir, citypair)\n",
    "        for foldertype in ['classification, frequencies', 'lemmatisation']:\n",
    "            os.makedirs(f\"{citypair_dir}/{foldertype}\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038bf9e2",
   "metadata": {},
   "source": [
    "### Folder Renaming function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc37c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = '../../../../../data/clean/city_pair_paragraphs/'\n",
    "for f in os.listdir(p)[:4]:\n",
    "    for citypair in os.listdir(os.path.join(p, f)):\n",
    "        citypair_path = os.path.join(os.path.join(p,f), citypair)\n",
    "        newname_path =   os.path.join(os.path.join(p,f), f\"cities___{citypair}___\")\n",
    "        os.rename(citypair_path, newname_path)\n",
    "        print(citypair)\n",
    "    #print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0746e52",
   "metadata": {},
   "source": [
    "### Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6aa934a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1451cc4fe12146319ad32cb6d6a40eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5474d0531a4c7caa6de7caf9de3772",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ca2d285643475287ad2e5a998b5ed7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "berlin_milan.csv berlin_milan.csv\n",
      "True ../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___berlin_milan___/lemmatisation\\ADJ__berlin_milan__.pickle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6478fe32631344549b07d97d1830df95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "237159d4492846d78a2cca408b19dc69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN__berlin_milan__.pickle berlin_milan.csv\n",
      "VERB__berlin_milan__.pickle berlin_milan.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b900b832917645729b2e9886d5a3ad90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "london_berlin.csv london_berlin.csv\n",
      "True ../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___london_berlin___/lemmatisation\\ADJ__london_berlin__.pickle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4fd61b088f149c38fa9fa5302aa3106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c94eaa301c454909a3a0d9b741fc2a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN__london_berlin__.pickle london_berlin.csv\n",
      "VERB__london_berlin__.pickle london_berlin.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7150e312e5114fc889caa9f2636d6269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "london_madrid.csv london_madrid.csv\n",
      "True ../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___london_madrid___/lemmatisation\\ADJ__london_madrid__.pickle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26985b69a714eb4958a12397ce25815",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6512184b9b482d98225a7465d230dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN__london_madrid__.pickle london_madrid.csv\n",
      "VERB__london_madrid__.pickle london_madrid.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9ab75fdac749ecb5cc0af78d8900ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "london_milan.csv london_milan.csv\n",
      "True ../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___london_milan___/lemmatisation\\ADJ__london_milan__.pickle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb326831ca343c590be6fa45f9f2695",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1927d135c82a4214aae92c585869680b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN__london_milan__.pickle london_milan.csv\n",
      "VERB__london_milan__.pickle london_milan.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38605053bea74385a0705dad63052ddb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "madrid_berlin.csv madrid_berlin.csv\n",
      "True ../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___madrid_berlin___/lemmatisation\\ADJ__madrid_berlin__.pickle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7048714df1b4a9d9adedd06e25823d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5942a27976254101a574bd60e5e8e57c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN__madrid_berlin__.pickle madrid_berlin.csv\n",
      "VERB__madrid_berlin__.pickle madrid_berlin.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19db73d9068b4811b83bdb6afa4d7fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "madrid_milan.csv madrid_milan.csv\n",
      "True ../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___madrid_milan___/lemmatisation\\ADJ__madrid_milan__.pickle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1074ed0baebf4dcba87299728db59f44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9f1a7080864d419db52b98365810e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN__madrid_milan__.pickle madrid_milan.csv\n",
      "VERB__madrid_milan__.pickle madrid_milan.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e684e0507c004a6d8c48a057c235c1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris_berlin.csv paris_berlin.csv\n",
      "True ../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___paris_berlin___/lemmatisation\\ADJ__paris_berlin__.pickle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7683d8ede42448db8e8f90f3369133a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "297d7fe8498d45f7a32e5c1a40c2db3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN__paris_berlin__.pickle paris_berlin.csv\n",
      "VERB__paris_berlin__.pickle paris_berlin.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a3d81a794e4408a743f1c25b13a39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris_london.csv paris_london.csv\n",
      "True ../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___paris_london___/lemmatisation\\ADJ__paris_london__.pickle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e35660977642a8a64e7c42112a8a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8333a3090404424f8ce1cf0bee135b58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN__paris_london__.pickle paris_london.csv\n",
      "VERB__paris_london__.pickle paris_london.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a28c603d1cdb4002a1eef0215a710e17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris_madrid.csv paris_madrid.csv\n",
      "True ../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___paris_madrid___/lemmatisation\\ADJ__paris_madrid__.pickle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37bd63d7cbe14f459fe1829a82b382e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4df8e0c8202476890d4c756cd9ca7d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN__paris_madrid__.pickle paris_madrid.csv\n",
      "VERB__paris_madrid__.pickle paris_madrid.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891294dfc472426787ed22687622e7cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paris_milan.csv paris_milan.csv\n",
      "True ../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___paris_milan___/lemmatisation\\ADJ__paris_milan__.pickle\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505026c714504ad69eb8721a12df9516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9932d4c338924cd9addc00803795458d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOUN__paris_milan__.pickle paris_milan.csv\n",
      "VERB__paris_milan__.pickle paris_milan.csv\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "batches = [5, 10, 20, 30]\n",
    "POS = ['VERB, NOUN, ADJ']\n",
    "\n",
    "\n",
    "INPUT_DIR = \"../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/\"\n",
    "POS = 'ADJ'\n",
    "lemmatise_multiple_files(INPUT_DIR, POS, OVERWRITE_PROTECTION=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1898abc",
   "metadata": {},
   "source": [
    "### Remove non-existent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3316aa07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english_word(word, english_words):\n",
    "    return word.lower() in english_words\n",
    "\n",
    "\n",
    "\n",
    "def remove_non_existing_words(wordlist: list, english_words) -> list:\n",
    "    wordset = set(wordlist)\n",
    "    non_existent = []\n",
    "    \n",
    "    for word in wordset:\n",
    "        if not is_english_word(word, english_words):\n",
    "            non_existent.append(word)\n",
    "    # print(non_existent)\n",
    "    return([word for word in wordlist if word not in non_existent])\n",
    "\n",
    "\n",
    "\n",
    "def remove_non_existing_words_for_dir(INPUT_DIR, OUTPUT_DIR, english_words_file=\"../../../input/english_words_alpha_370k.txt\"):\n",
    "    # DIR = \"../../../../data/enwiki_city_pairs_lemmatised/VERBNOUNADJ_SPACY_LARGE\"\n",
    "    # NEWDIR = \"../../../../data/enwiki_city_pairs_lemmatised/VERBNOUNADJ_SPACY_LARGE_REAL_WORDS\"\n",
    "        \n",
    "\n",
    "    for path in [INPUT_DIR, OUTPUT_DIR, english_words_file]:\n",
    "        if not os.path.exists(path):\n",
    "            raise Exception(\"{path} is not a valid path.\")\n",
    "\n",
    "    with open(english_words_file) as word_file:\n",
    "        english_words = set(word.strip().lower() for word in word_file)\n",
    "\n",
    "    for file in os.listdir(INPUT_DIR):\n",
    "        file_path = os.path.join(INPUT_DIR, file)\n",
    "        with open(file_path, 'rb') as fp:\n",
    "            wordlist = pickle.load(fp)\n",
    "        \n",
    "        cleaned_wordlist = remove_non_existing_words(wordlist, english_words)\n",
    "   \n",
    "        with open(os.path.join(OUTPUT_DIR, file), 'wb') as fp2:\n",
    "            pickle.dump(cleaned_wordlist, fp2)\n",
    "        \n",
    "        print('old:', len(wordlist), 'new:', len(cleaned_wordlist), 'removed:', len(wordlist)-len(cleaned_wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5e5c0f9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1598796255.py, line 42)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [116]\u001b[1;36m\u001b[0m\n\u001b[1;33m    f\"{city_pair_dir}/)\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "def removing_non_existent_words(file_path, english_words):\n",
    "    if not 'CLEAN' in file_path:\n",
    "        with open(file_path, 'rb') as fp:\n",
    "            lemmatised_paragraphs = pickle.load(fp)\n",
    "\n",
    "            for paragraph_id in lemmatised_paragraphs:\n",
    "                cleaned_lemmatised_paragraph = remove_non_existing_words(lemmatised_paragraphs[paragraph_id], english_words)\n",
    "                lemmatised_paragraphs[paragraph_id] = cleaned_lemmatised_paragraph\n",
    "\n",
    "        new_file_path = file_path.replace('.pickle, CLEAN.pickle')\n",
    "        with open(new_file_path, 'wb') as fp:\n",
    "            pickle.dump(lemmatised_paragraphs, fp)\n",
    "            \n",
    "            \n",
    "\n",
    "def process_batch(PATH='../../../../../data/clean/city_pair_paragraphs/', BATCHES=[5], POS=[], NON_EXISTENT_WORD_REMOVAL=True,\n",
    "                 english_words_file=\"../../../input/english_words_alpha_370k.txt\"):\n",
    "    BATCHES = [int(x) for x in BATCHES]\n",
    "    \n",
    "    with open(english_words_file) as word_file:\n",
    "        english_words = set(word.strip().lower() for word in word_file)\n",
    "\n",
    "    i=0\n",
    "    for root, dirs, files in os.walk(PATH, topdown=True):\n",
    "        for name in files:\n",
    "            reg_str = 'biggest_cities_([0-9]+)'\n",
    "            parent_dir = int(re.findall(reg_str, root)[0])\n",
    "            \n",
    "            if not BATCHES or parent_dir in BATCHES:\n",
    "                file_path = os.path.join(root, name)\n",
    "                \n",
    "                \n",
    "                if NON_EXISTENT_WORD_REMOVAL and os.path.basename(root) == 'lemmatisation':\n",
    "                    i+= 1\n",
    "                    # removing_non_existent_words(file_path, english_words)\n",
    "    print(i)\n",
    "    \n",
    "#     for batch in os.listdir(INPUT_DIR):\n",
    "#         batch_dir = os.path.join(INPUT_DIR, batch)\n",
    "#             for citypair in os.listdir(batch_dir):\n",
    "#                 citypair_dir = os.path.join(batch_dir, citypair)\n",
    "                for file in os.list(f\"{city_pair_dir}/lemmatisation\"):\n",
    "                    # check which POS tags are missing\n",
    "                    # lemmatise\n",
    "                    #for tag in POS:\n",
    "                        \n",
    "    \n",
    "\n",
    "process_batch()        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2712144",
   "metadata": {},
   "source": [
    "### Import Lemmatised Paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0dfc0f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_lemmatised_wordlists(PATH, POS,  BATCHES=[], english_words_only=True):\n",
    "\n",
    "    \"\"\"\n",
    "    -->\n",
    "        function that imports (POS specific) wordlists belonging to specific city pairs.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "            PATH: str -> path to lemmatised wordlists (e.g. '../../../../data/enwiki_city_pairs_lemmatised/NOUN/')\n",
    "            sort: bool (default = True) -> sort based on the number of city pair co-occurences\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.isdir(PATH):\n",
    "        raise Exception(\"Path is incorrect.\")\n",
    "\n",
    "    POStags=[\"PROPN\", \"AUX\", \"NOUN\", \"ADJ\", \"VERB\", \"ADP\", \"SYM\", \"NUM\"]\n",
    "    \n",
    "    #Checks if valid part-of-speech list was provided\n",
    "    for tag in POS:\n",
    "        if not isinstance(POS, list) or tag.upper() not in POStags:\n",
    "            raise Exception(f'POSfilter only allows a list with one or multiple from the following tags: {POStags}.')\n",
    "    \n",
    "    BATCHES = [int(x) for x in BATCHES]\n",
    "    datadict= {}\n",
    "    \n",
    "    for root, dirs, files in os.walk(PATH, topdown=True):\n",
    "        for name in files:\n",
    "            reg_str = 'biggest_cities_([0-9]+)'\n",
    "            parent_dir = int(re.findall(reg_str, root)[0])\n",
    "            \n",
    "            if not BATCHES or parent_dir in BATCHES:\n",
    "                CITY_PAIR = root.split('___')[1]\n",
    "                \n",
    "                if CITY_PAIR not in datadict.keys():\n",
    "                    datadict[CITY_PAIR] = {'batch': parent_dir, 'paragraphs': None}\n",
    "                \n",
    "                for tag in POS:\n",
    "                    if tag in name and ('CLEANED' in name or not english_words_only): # any(tag for tag in POS in name):\n",
    "                        file_path = os.path.join(root, name)\n",
    "                        \n",
    "                        with open(file_path, 'rb') as fp:\n",
    "                            lemmatised_paragraphs = pickle.load(fp)\n",
    "\n",
    "                            if datadict[CITY_PAIR]['paragraphs'] is None:\n",
    "                                datadict[CITY_PAIR]['paragraphs'] = len(lemmatised_paragraphs.keys())\n",
    "\n",
    "                            datadict[CITY_PAIR][tag] = lemmatised_paragraphs\n",
    "                        \n",
    "    # Check if all lemmatisation files were present\n",
    "    missing = {k: [] for k in POS} \n",
    "    for citypair in datadict.keys():\n",
    "        for tag in POS:\n",
    "            if tag not in datadict[citypair]:\n",
    "                missing[tag].append(citypair)\n",
    "    \n",
    "    for k in missing:\n",
    "        if len(missing[k]):\n",
    "            if english_words_only:\n",
    "                print(\"Possible the lemmatised paragraphs haven't been filtered for english words only yet.\", end=\" \")\n",
    "            print(f\"Missing '{k}' files for: \\n--> {missing[k]}\\n\")\n",
    "            \n",
    "    print(f'\\n Getting lemmatised paragraphs for {len(datadict.keys())} city pairs...')\n",
    "    \n",
    "    return datadict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "72ebe9a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible the lemmatised paragraphs haven't been filtered for english words only yet. Missing 'VERB' files for: \n",
      "--> ['berlin_milan', 'london_berlin', 'london_madrid', 'london_milan', 'madrid_berlin', 'madrid_milan', 'paris_berlin', 'paris_london', 'paris_madrid', 'paris_milan']\n",
      "\n",
      "Possible the lemmatised paragraphs haven't been filtered for english words only yet. Missing 'NOUN' files for: \n",
      "--> ['berlin_milan', 'london_berlin', 'london_madrid', 'london_milan', 'madrid_berlin', 'madrid_milan', 'paris_berlin', 'paris_london', 'paris_madrid', 'paris_milan']\n",
      "\n",
      "Possible the lemmatised paragraphs haven't been filtered for english words only yet. Missing 'ADJ' files for: \n",
      "--> ['berlin_milan', 'london_berlin', 'london_madrid', 'london_milan', 'madrid_berlin', 'madrid_milan', 'paris_berlin', 'paris_london', 'paris_madrid', 'paris_milan']\n",
      "\n",
      "Possible the lemmatised paragraphs haven't been filtered for english words only yet. Missing 'AUX' files for: \n",
      "--> ['berlin_milan', 'london_berlin', 'london_madrid', 'london_milan', 'madrid_berlin', 'madrid_milan', 'paris_berlin', 'paris_london', 'paris_madrid', 'paris_milan']\n",
      "\n",
      "\n",
      " Getting lemmatised paragraphs for 10 city pairs...\n"
     ]
    }
   ],
   "source": [
    "BATCHES=[5]\n",
    "POS=['VERB', 'NOUN', 'ADJ', \"AUX\"]\n",
    "\n",
    "a = import_lemmatised_wordlists(PATH='../../../../../data/clean/city_pair_paragraphs/', BATCHES=BATCHES, POS=POS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e54840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69860f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for root, dirs, files in tqdm(list(os.walk(INPUT_DIR))):\n",
    "# #     dirs = []\n",
    "#     for file in files:\n",
    "#         file_path = os.path.join(root, file)\n",
    "#         # file_output_dir = root.replace(INPUT_DIR, OUTPUT_DIR)+'/'+file[:-4]\n",
    "#         nested_dir = root + '/' + file[:-4]\n",
    "# #         dirs.append(file_output_dir)\n",
    "# #         print(nested_dir)\n",
    "#         os.makedirs(nested_dir, exist_ok=True)\n",
    "#         original = file_path\n",
    "#         target = nested_dir + '/' + file\n",
    "#         shutil.move(original, target)\n",
    "#     #[os.makedirs(dir, exist_ok=True) for dir in dirs]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
