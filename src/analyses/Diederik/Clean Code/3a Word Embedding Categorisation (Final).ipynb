{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ba24f61",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2acb2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing_functions import *\n",
    "from word_embedding_functions import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572bfe58",
   "metadata": {},
   "source": [
    "- Clean and preprocess (lemmatise) a list of documents (e.g. paragraphs)\n",
    "- Get your topics through unsupervised clustering with (LDA Topic Modeling)\n",
    "- Use these for the word embedding algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c3efed",
   "metadata": {},
   "source": [
    "# Topic Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "6ce2990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing words with double meaning helps (e.g. bear, degree)\n",
    "\n",
    "#fashion_words = ['fashion', 'model', 'vogue', 'store', 'designer', 'couture', 'catwalk', 'runway', 'modeling', 'clothing', 'cosmetic', 'brand', 'retail', 'advertising', 'perfume']\n",
    "diplomacy_words = ['War', 'Embassy', 'Army', 'Diplomatic', 'Ambassador', 'Treaty', 'Protest', 'Force', 'Mission', 'Arrest', 'Government', 'Police', 'Attack', 'Party', 'Minister']\n",
    "entertainment_words = ['Opera', 'Festival', 'Perform', 'Orchestra', 'Symphony', 'Concert', 'Music', 'Film', 'Sing', 'Theatre', 'Performance', 'Role', 'Premiere', 'Tour', 'Band']\n",
    "art_words = ['Exhibition', 'Art', 'Museum', 'Gallery', 'Exhibit', 'Painting', 'Collection', 'Paint', 'Portrait', 'Artist', 'Sculpture', 'Fashion', 'Design', 'Contemporary', 'Painter']\n",
    "# education_words = ['Study', 'School', 'Professor', 'University', 'Graduate', 'Educate', 'Lecture', 'Research', 'College', 'Teach', 'Science', 'Education', 'Philosophy', 'Doctorate', 'Faculty'] # replaced bear with educate and degree with Lecture\n",
    "transportation_words = ['Railway', 'Route', 'Line', 'Operate', 'Flight', 'Station', 'Service', 'Airline', 'Airport', 'Train', 'Passenger', 'Speed', 'Aircraft', 'Rail', 'Network']\n",
    "sport_words = ['Final', 'Win', 'Team', 'Match', 'Game', 'Goal', 'Club', 'League', 'Champion', 'Championship', 'Season', 'Score', 'Round', 'Tournament', 'Football']\n",
    "\n",
    "lda_topic_words = { #'fashion': fashion_words,\n",
    "                    'diplomacy': diplomacy_words,\n",
    "                   'entertainment': entertainment_words,\n",
    "                   'art': art_words,\n",
    "                   # 'education': education_words,\n",
    "                   'transportation': transportation_words,\n",
    "                   'sport': sport_words}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7acd430",
   "metadata": {},
   "source": [
    "### 1. Loading GloVe Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "03213f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_word_embeddings(GLOVE_PATH='.../.../.../.../../glove.42B.300d.txt'):\n",
    "    \"\"\"\n",
    "    --> Function that loads glove word embeddings.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "            GLOVE_PATH: Str -> Path to the GloVe file\n",
    "\n",
    "    \"\"\"\n",
    "    if not os.path.exists(GLOVE_PATH):\n",
    "        raise Exception(\"The given PATH to the GloVe file doesn't exist.\")\n",
    "    \n",
    "    \n",
    "    embeddings_dict = {}\n",
    "    discarded_dict = {}\n",
    "    print('This will take approximately ~ 4 minutes...')\n",
    "\n",
    "    num_lines = sum(1 for line in open(GLOVE_PATH,'r', encoding=\"utf-8\"))\n",
    "\n",
    "    with open(GLOVE_PATH, 'r', encoding=\"utf-8\") as f:\n",
    "        for line in tqdm(f, total=num_lines):\n",
    "            values = line.split()\n",
    "            token = values[0]\n",
    "            try:\n",
    "                vector = np.asarray(values[1:], \"float32\")\n",
    "                if vector.shape[0] == 300:\n",
    "                    embeddings_dict[token] = vector\n",
    "                else:\n",
    "                    discarded_dict[token] = vector\n",
    "            except:\n",
    "                discarded_dict[token] = None\n",
    "    \n",
    "    return embeddings_dict, discarded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3b8c87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will take approximately ~ 4 minutes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b946f1fe024ec381fe4db554a97c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2196017 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 26s\n",
      "Wall time: 4min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embeddings_dict, discarded_dict = load_glove_word_embeddings(GLOVE_PATH=\"../../../../../glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b381a8",
   "metadata": {},
   "source": [
    "### 2. Get Mean of the words from each Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "a006cfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_vectors_dict = {}\n",
    "for topic in lda_topic_words:\n",
    "    words = lda_topic_words[topic]\n",
    "    words = [word for word in words if word in embeddings_dict.keys()] # checks if word is in vocabulary (i.e. has been seen by the model before)\n",
    "    mean_embedding = np.mean([embeddings_dict[word] for word in words], axis=0)\n",
    "    mean_vectors_dict[topic] = mean_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739fefaf",
   "metadata": {},
   "source": [
    "#### Show closest words to topic vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4657f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "from preprocessing_functions import *\n",
    "\n",
    "def find_closest_embeddings(embedding, cutoff=25):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda token: spatial.distance.euclidean(embeddings_dict[token], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "79ce6883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "topic = 'fashion'\n",
    "\n",
    "words = find_closest_embeddings(embedding=\n",
    "     mean_vectors_dict[topic]    # embeddings_dict['diplomacy'] # embeddings_dict['fashion']\n",
    ")[:2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "1c653f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_words = get_english_words(path='../../../input/english_words_alpha_370k.txt\n",
    "# print(remove_non_existing_words_from_wordlist(words, english_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e451b05",
   "metadata": {},
   "source": [
    "### 3. Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2794ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "df = pd.read_csv(\"../../../../../data/clean/classified_435_citypairs_311k_paragraphs.csv\")\n",
    "df['merged_POS'] = df['merged_POS'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36d1f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def categorize_text(lemmatized_wordlist, mean_vectors_dict, keywords, embeddings_dict, bottom_threshold=0.1, verbose1=False, verbose2=False):\n",
    "#     \"\"\"\n",
    "#     --> Function that loads glove word embeddings.\n",
    "\n",
    "#         Parameters:\n",
    "#         -----------\n",
    "#             lemmatized_words: List -> List of words.\n",
    "#             keywords: nested list -> List of lists of keywords that represent categories.\n",
    "#             number_of_keywords: Int (default = 1) -> number of keywords to use from a category (setting it to 0 will use them all!)\n",
    "#             bottom_threshold: Float (default = 0.1) -> Lowest allowed similarity value between a word and dominant category.\n",
    "#             verbose1: Bool (default = False) -> Shows similarity calculations between a word and each keyword.\n",
    "#             verbose2: Bool (default = False) -> Shows similarity calculations between a word and each category.\n",
    "\n",
    "#     \"\"\"\n",
    "#     similar_categories = []\n",
    "    \n",
    "#     for word in lemmatized_wordlist:\n",
    "#         try:\n",
    "#             word_vector = embeddings_dict[word]\n",
    "#         except:\n",
    "#             continue\n",
    "            \n",
    "#         if verbose1 or verbose2:\n",
    "#             print(f\"word: \\t\\t'{word}'\")\n",
    "\n",
    "#         closeness = []\n",
    "\n",
    "#         for category in keywords:\n",
    "#             try:\n",
    "#                 keyword_vector = mean_vectors_dict[category]\n",
    "#             except:\n",
    "#                 keyword_vector = embeddings_dict[category]\n",
    "                \n",
    "#             similarity = 1 - cosine(keyword_vector, word_vector)\n",
    "\n",
    "#             closeness.append((similarity, category))\n",
    "\n",
    "#             if verbose1:\n",
    "#                 # print('___________________________')\n",
    "#                 print('===>', '\\t\\tcategory:', category, '\\n\\t\\tsimilarity:', similarity, f\"\\n\")\n",
    "\n",
    "\n",
    "#         similar_category = max(closeness)\n",
    "\n",
    "#         sortedcat = sorted(closeness, key=lambda item: item[0], reverse=True)\n",
    "#         if (sortedcat[0][0] - 0.05) > sortedcat[2][0]:\n",
    "#             allowed = True\n",
    "#         else:\n",
    "#             allowed = False\n",
    "\n",
    "\n",
    "\n",
    "#         if similar_category[0] > bottom_threshold and allowed:\n",
    "#             similar_categories.append((word, similar_category))\n",
    "#             if verbose2:\n",
    "#                 #print('category similarity:')\n",
    "#                 # pprint(sorted(closeness, key=lambda x: x[0], reverse=True))\n",
    "#                 print(f\"choice: \\tkept\")\n",
    "#                 print(f\"\\n==> \\tcategory:, {similar_category[1]}, \\n\\tsimilarity score: {similar_category[0]}\")\n",
    "#         elif verbose2:\n",
    "#             print(f\"choice: \\tdiscarded\")\n",
    "#             reason = 'ambiguity' if not allowed else 'low similarity score'\n",
    "#             print(f\"reasoning: \\t{reason}\")\n",
    "\n",
    "#         if verbose2:\n",
    "#             # print(f\"analysis: {'not' if not allowed else ''} enough difference\\nscores:\")\n",
    "#             print(\"\\nscores:\\n\\t----category----          ----score----\")\n",
    "#             for i in sortedcat:\n",
    "#                 print(f\"\\t{i[1]:<10s} \\t\\t{i[0]}\")\n",
    "#             print()\n",
    "\n",
    "#             print('='*100)\n",
    "#             print('')\n",
    "    \n",
    "#     categories_dict = {key: 0 for key in keywords}\n",
    "\n",
    "#     for x in similar_categories:\n",
    "#         categories_dict[x[1][1]] += x[1][0] #print(x[1])\n",
    "    \n",
    "\n",
    "#     nonsorted_results = list(sorted(categories_dict.items(), key=lambda item: item[0], reverse=False))\n",
    "#     results = list(sorted(categories_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "#     #pprint(results)\n",
    "\n",
    "#     # print(f\"\\nThe dominant category is: '{results[0][0]}'\", end='')\n",
    "#     #if (results[0][1] - (float(results[0][1])/5)) <= results[1][1]:  \n",
    "#     #    print(f\", closely followed by: '{results[1][0]}'.\")\n",
    "#     if verbose2:\n",
    "#         print('\\n')\n",
    "#         pprint(similar_categories)\n",
    "#     # print('\\n --------------------------------------------------------------------')\n",
    "    \n",
    "#     prediction_dict = {'category_similarities': nonsorted_results, 'prediction': results[0][0]} \n",
    "\n",
    "#     return prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "a7cef080",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df[df['idxmax'] != 'Education'][:5000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4471c8",
   "metadata": {},
   "source": [
    "### 3. Classify Paragraphs (by Word Embedding Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65deda",
   "metadata": {},
   "source": [
    "#### 3.1 Select right parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "77d77c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_threshold = 0.20\n",
    "verbose1 = False\n",
    "verbose2 = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "d9d2ba63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1.98 ms\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097ca87743c045d0967849eb784824ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "topics = [key for key in list(sorted(mean_vectors_dict.keys()))]\n",
    "nested_l = [['index']+topics+['outcome']]\n",
    "\n",
    "for idx, row in tqdm(sample['merged_POS'].iteritems(), total=len(sample['merged_POS'])):\n",
    "    output = categorize_text(lemmatized_wordlist=row, mean_vectors_dict=mean_vectors_dict, keywords=topics, embeddings_dict=embeddings_dict, bottom_threshold=bottom_threshold, verbose1=verbose1, verbose2=verbose2)\n",
    "    \n",
    "    temp_l = [idx] +[result[1] for result in output['category_similarities']] + [output['prediction']]\n",
    "    \n",
    "    #print(row, output['category_similarities'])\n",
    "    \n",
    "    if len(nested_l[0]) != len(temp_l):\n",
    "        raise Exception('Not the same size!')\n",
    "        \n",
    "    nested_l.append(temp_l)\n",
    "\n",
    "\n",
    "prediction_df = pd.DataFrame(nested_l[1:],columns=nested_l[0]).set_index('index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "61a1d113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>art</th>\n",
       "      <th>diplomacy</th>\n",
       "      <th>entertainment</th>\n",
       "      <th>sport</th>\n",
       "      <th>transportation</th>\n",
       "      <th>outcome</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.326078</td>\n",
       "      <td>0.301628</td>\n",
       "      <td>0.768217</td>\n",
       "      <td>0.609732</td>\n",
       "      <td>diplomacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.26583</td>\n",
       "      <td>1.243361</td>\n",
       "      <td>0.322190</td>\n",
       "      <td>0.425173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>diplomacy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.893548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.217850</td>\n",
       "      <td>3.286044</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.75122</td>\n",
       "      <td>0.405813</td>\n",
       "      <td>0.301628</td>\n",
       "      <td>0.752151</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.335578</td>\n",
       "      <td>transportation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           art  diplomacy  entertainment     sport  transportation  \\\n",
       "index                                                                \n",
       "0      0.00000   2.326078       0.301628  0.768217        0.609732   \n",
       "1      0.26583   1.243361       0.322190  0.425173        0.000000   \n",
       "2      0.00000   0.893548       0.000000  0.217850        3.286044   \n",
       "3      0.75122   0.405813       0.301628  0.752151        0.000000   \n",
       "4      0.00000   0.000000       0.000000  0.000000        0.335578   \n",
       "\n",
       "              outcome  \n",
       "index                  \n",
       "0           diplomacy  \n",
       "1           diplomacy  \n",
       "2      transportation  \n",
       "3               sport  \n",
       "4      transportation  "
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b59cea0",
   "metadata": {},
   "source": [
    "### 4. Insert classification into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "15818543",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df = sample.join(prediction_df)\n",
    "updated_df.head(2)\n",
    "updated_df['same_categorisation'] = (updated_df['idxmax'].str.lower() == updated_df['outcome'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafe7bd6",
   "metadata": {},
   "source": [
    "#### Similarity between lda topic model and word embedding algorithm output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "6651f666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4102"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df['same_categorisation'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "43588af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.0, 20.0]      611\n",
       "(20.0, 30.0]      544\n",
       "(30.0, 40.0]      426\n",
       "(-0.001, 10.0]    338\n",
       "Name: merged_POS, dtype: int64"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of lemmatised words in the paragraphs\n",
    "updated_df[(updated_df['max'] > 0.9)]['merged_POS'].str.len().value_counts(bins=[0, 10, 20, 30, 40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "38915569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idxmax          same_categorisation\n",
      "Art             True                   471\n",
      "                False                   86\n",
      "Diplomacy       True                   419\n",
      "                False                   18\n",
      "Entertainment   True                   957\n",
      "                False                   23\n",
      "Sport           True                   176\n",
      "                False                    5\n",
      "Transportation  True                   201\n",
      "                False                   83\n",
      "Name: same_categorisation, dtype: int64\n",
      "----------------------------------------------------\n",
      "idxmax          same_categorisation\n",
      "Art             True                   0.845601\n",
      "                False                  0.154399\n",
      "Diplomacy       True                   0.958810\n",
      "                False                  0.041190\n",
      "Entertainment   True                   0.976531\n",
      "                False                  0.023469\n",
      "Sport           True                   0.972376\n",
      "                False                  0.027624\n",
      "Transportation  True                   0.707746\n",
      "                False                  0.292254\n",
      "Name: same_categorisation, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(updated_df[(updated_df['max'] > 0.9) & (updated_df['merged_POS'].str.len() > 5)].groupby('idxmax')['same_categorisation'].value_counts())\n",
    "print('----------------------------------------------------')\n",
    "print(updated_df[(updated_df['max'] > 0.9) & (updated_df['merged_POS'].str.len() > 5)].groupby('idxmax')['same_categorisation'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fc731c",
   "metadata": {},
   "source": [
    "### 5. Aggregate paragraphs Classification into City Pair Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0dfff1",
   "metadata": {},
   "source": [
    "#### 5.1 Select right parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "04f99088",
   "metadata": {},
   "outputs": [],
   "source": [
    "bottom_lda_threshold = 0.9\n",
    "minimal_paragraph_length = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "55fa4999",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = updated_df[(updated_df['city_pair'].isin(updated_df['city_pair'].unique()[:])) & (updated_df['max'] > bottom_lda_threshold) & (updated_df['merged_POS'].str.len() > minimal_paragraph_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "7c995765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df[temp_df['idxmax'] != 'Education']['same_categorisation'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "bf26ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = temp_df.groupby('city_pair')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "bf577f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_list2 = [['city_pair', 'paragraphs', 'lemmatised_paragraph_length', 'same_categorisation_raw', 'same_categorisation_percentage',\n",
    "                'lda_dominant_category', 'embedding_dominant_category', 'lda_art', 'embedding_art', 'lda_diplomacy',\n",
    "                'embedding_diplomacy', 'lda_entertainment', 'embedding_entertainment', \n",
    "                'lda_sport', 'embedding_sport', 'lda_transportation', 'embedding_transportation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "a01981ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9a632b078546219c3994ebf074b5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "categories = topics\n",
    "for city_pair, sub_df in tqdm(grouped_df):\n",
    "    paragraph_count = sub_df['paragraph'].count()\n",
    "    same_categorisation_raw = sub_df['same_categorisation'].sum()\n",
    "    same_categorisation_percentage = sub_df['same_categorisation'].sum()/sub_df['same_categorisation'].count()\n",
    "    lemmatised_paragraph_len = sub_df['merged_POS'].str.len().mean()\n",
    "    \n",
    "    lda_prediction = sub_df['idxmax'].value_counts()\n",
    "    embedding_prediction = sub_df['outcome'].value_counts()\n",
    "        \n",
    "    lda_dominant_category = lda_prediction.idxmax()\n",
    "    embedding_dominant_category = embedding_prediction.idxmax()\n",
    "    \n",
    "    lda_prediction = lda_prediction.to_dict()\n",
    "    embedding_prediction = embedding_prediction.to_dict()\n",
    "    \n",
    "    if (len(lda_prediction) != 6) or (len(embedding_prediction) != 6):\n",
    "        for category in categories:\n",
    "            if category.capitalize() not in lda_prediction.keys():\n",
    "                lda_prediction[category.capitalize()] = 0\n",
    "            if category not in embedding_prediction.keys():\n",
    "                embedding_prediction[category] = 0\n",
    "                \n",
    "    temp_l2 = [city_pair, paragraph_count, lemmatised_paragraph_len, same_categorisation_raw, same_categorisation_percentage,\n",
    "                lda_dominant_category, embedding_dominant_category, lda_prediction['Art'], embedding_prediction['art'], lda_prediction['Diplomacy'],\n",
    "                embedding_prediction['diplomacy'],\n",
    "                lda_prediction['Entertainment'], embedding_prediction['entertainment'], lda_prediction['Sport'],\n",
    "                embedding_prediction['sport'], lda_prediction['Transportation'], embedding_prediction['transportation']]\n",
    "    if len(nested_list2[0]) != len(temp_l2):\n",
    "        raise Exception('Not the same size!')\n",
    "    \n",
    "    nested_list2.append(temp_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "bd6d9c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lda_art</th>\n",
       "      <th>embedding_art</th>\n",
       "      <th>lda_diplomacy</th>\n",
       "      <th>embedding_diplomacy</th>\n",
       "      <th>lda_entertainment</th>\n",
       "      <th>embedding_entertainment</th>\n",
       "      <th>lda_sport</th>\n",
       "      <th>embedding_sport</th>\n",
       "      <th>lda_transportation</th>\n",
       "      <th>embedding_transportation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.192771</td>\n",
       "      <td>0.174699</td>\n",
       "      <td>0.106426</td>\n",
       "      <td>0.134538</td>\n",
       "      <td>0.479920</td>\n",
       "      <td>0.481928</td>\n",
       "      <td>0.068273</td>\n",
       "      <td>0.090361</td>\n",
       "      <td>0.152610</td>\n",
       "      <td>0.118474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.237506</td>\n",
       "      <td>0.200412</td>\n",
       "      <td>0.197836</td>\n",
       "      <td>0.235446</td>\n",
       "      <td>0.381762</td>\n",
       "      <td>0.385368</td>\n",
       "      <td>0.075734</td>\n",
       "      <td>0.091705</td>\n",
       "      <td>0.107161</td>\n",
       "      <td>0.087069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lda_art  embedding_art  lda_diplomacy  embedding_diplomacy  \\\n",
       "0  0.192771       0.174699       0.106426             0.134538   \n",
       "1  0.237506       0.200412       0.197836             0.235446   \n",
       "\n",
       "   lda_entertainment  embedding_entertainment  lda_sport  embedding_sport  \\\n",
       "0           0.479920                 0.481928   0.068273         0.090361   \n",
       "1           0.381762                 0.385368   0.075734         0.091705   \n",
       "\n",
       "   lda_transportation  embedding_transportation  \n",
       "0            0.152610                  0.118474  \n",
       "1            0.107161                  0.087069  "
      ]
     },
     "execution_count": 374,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.DataFrame(nested_list2[1:],columns=nested_list2[0])\n",
    "\n",
    "# Normalize category outcomes\n",
    "final_df[list(final_df.columns)[7:]].div(final_df['paragraphs'], axis=0) # .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "d56b1d78",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_pair</th>\n",
       "      <th>paragraphs</th>\n",
       "      <th>lemmatised_paragraph_length</th>\n",
       "      <th>same_categorisation_raw</th>\n",
       "      <th>same_categorisation_percentage</th>\n",
       "      <th>lda_dominant_category</th>\n",
       "      <th>embedding_dominant_category</th>\n",
       "      <th>lda_art</th>\n",
       "      <th>embedding_art</th>\n",
       "      <th>lda_diplomacy</th>\n",
       "      <th>embedding_diplomacy</th>\n",
       "      <th>lda_entertainment</th>\n",
       "      <th>embedding_entertainment</th>\n",
       "      <th>lda_sport</th>\n",
       "      <th>embedding_sport</th>\n",
       "      <th>lda_transportation</th>\n",
       "      <th>embedding_transportation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>berlin_milan</td>\n",
       "      <td>498</td>\n",
       "      <td>29.405622</td>\n",
       "      <td>458</td>\n",
       "      <td>0.919679</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>96</td>\n",
       "      <td>87</td>\n",
       "      <td>53</td>\n",
       "      <td>67</td>\n",
       "      <td>239</td>\n",
       "      <td>240</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>76</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>london_berlin</td>\n",
       "      <td>1941</td>\n",
       "      <td>32.223596</td>\n",
       "      <td>1766</td>\n",
       "      <td>0.909840</td>\n",
       "      <td>Entertainment</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>461</td>\n",
       "      <td>389</td>\n",
       "      <td>384</td>\n",
       "      <td>457</td>\n",
       "      <td>741</td>\n",
       "      <td>748</td>\n",
       "      <td>147</td>\n",
       "      <td>178</td>\n",
       "      <td>208</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       city_pair  paragraphs  lemmatised_paragraph_length  \\\n",
       "0   berlin_milan         498                    29.405622   \n",
       "1  london_berlin        1941                    32.223596   \n",
       "\n",
       "   same_categorisation_raw  same_categorisation_percentage  \\\n",
       "0                      458                        0.919679   \n",
       "1                     1766                        0.909840   \n",
       "\n",
       "  lda_dominant_category embedding_dominant_category  lda_art  embedding_art  \\\n",
       "0         Entertainment               entertainment       96             87   \n",
       "1         Entertainment               entertainment      461            389   \n",
       "\n",
       "   lda_diplomacy  embedding_diplomacy  lda_entertainment  \\\n",
       "0             53                   67                239   \n",
       "1            384                  457                741   \n",
       "\n",
       "   embedding_entertainment  lda_sport  embedding_sport  lda_transportation  \\\n",
       "0                      240         34               45                  76   \n",
       "1                      748        147              178                 208   \n",
       "\n",
       "   embedding_transportation  \n",
       "0                        59  \n",
       "1                       169  "
      ]
     },
     "execution_count": 375,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
