{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97317c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batch_dirs = [os.path.join(INPUT_DIR, batch) for batch in os.listdir(INPUT_DIR) if not BATCHES or int(re.findall(reg_str, batch)[0]) in BATCHES]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e12406d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def import_lemmatised_paragraphs(INPUT_DIR, POS, BATCHES=[], ONLY_ENGLISH_WORDS=False):\n",
    "    BATCHES = [int(x) for x in BATCHES]\n",
    "    reg_str = 'biggest_cities_([0-9]+)'\n",
    "    \n",
    "    batch_dirs = [batch for batch in os.listdir(INPUT_DIR) if not BATCHES or int(re.findall(reg_str, batch)[0]) in BATCHES]\n",
    "    \n",
    "    data_dict = {}\n",
    "    for batch_name in tqdm(batch_dirs, desc=f\"BATCHES: {BATCHES}\"):\n",
    "        batch_dir = os.path.join(INPUT_DIR, batch_name)\n",
    "        \n",
    "        for citypair in tqdm(os.listdir(batch_dir), desc=\"City Pair\", leave=False):\n",
    "            citypair_dir = os.path.join(batch_dir, citypair)\n",
    "            CITY_PAIR = citypair.split('___')[1]\n",
    "            \n",
    "            paragraphs_count = len(pd.read_csv(f\"{citypair_dir}/{CITY_PAIR}.csv\"))\n",
    "            data_dict[CITY_PAIR] = {'batch': batch_name, 'original_paragraphs': paragraphs_count, 'english_words': ONLY_ENGLISH_WORDS}\n",
    "            \n",
    "            \n",
    "            \n",
    "            for tag in POS:\n",
    "                if ONLY_ENGLISH_WORDS:\n",
    "                    file_path = f\"{citypair_dir}/lemmatisation/{tag}_CLEAN.pickle\"\n",
    "                else:\n",
    "                    file_path = f\"{citypair_dir}/lemmatisation/{tag}_CLEAN.pickle\"\n",
    "                \n",
    "                if os.path.exists(file_path):\n",
    "                    with open(file_path, 'rb') as fp:\n",
    "                        lemmatised_paragraphs = pickle.load(fp)\n",
    "\n",
    "                        data_dict[CITY_PAIR][tag] = lemmatised_paragraphs\n",
    "    \n",
    "    # Check if all lemmatisation files were present\n",
    "    missing = {k: [] for k in POS} \n",
    "    for citypair in data_dict.keys():\n",
    "        for tag in POS:\n",
    "            if tag not in data_dict[citypair]:\n",
    "                missing[tag].append(citypair)\n",
    "    \n",
    "    for k in missing:\n",
    "        if len(missing[k]):\n",
    "            print(f\"The following city pairs have missing '{k}' files: \\n--> {missing[k]}\\n\")\n",
    "            \n",
    "    print(f'\\n Getting lemmatised paragraphs for {len(data_dict.keys())} city pairs...')\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8adf8f20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "149ca6328eaa4acfa5ed54529f3986bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BATCHES: [10]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "City Pair:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following city pairs have missing 'VERB' files: \n",
      "--> ['athens_birmingham', 'athens_lisbon', 'athens_rome', 'barcelona_athens', 'barcelona_birmingham', 'barcelona_lisbon', 'barcelona_rome', 'berlin_athens', 'berlin_barcelona', 'berlin_birmingham', 'berlin_lisbon', 'berlin_rome', 'birmingham_lisbon', 'london_athens', 'london_barcelona', 'london_birmingham', 'london_lisbon', 'london_rome', 'madrid_athens', 'madrid_barcelona', 'madrid_birmingham', 'madrid_lisbon', 'madrid_rome', 'milan_athens', 'milan_barcelona', 'milan_birmingham', 'milan_lisbon', 'milan_rome', 'paris_athens', 'paris_barcelona', 'paris_birmingham', 'paris_lisbon', 'paris_rome', 'rome_birmingham', 'rome_lisbon']\n",
      "\n",
      "\n",
      " Getting lemmatised paragraphs for 35 city pairs...\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIR = \"../../../../../data/clean/city_pair_paragraphs/\"\n",
    "BATCHES = [10]\n",
    "POS = [\"VERB\"]\n",
    "# LEMMATISATION_TYPE = 'quick', 'accurate'\n",
    "ONLY_ENGLISH_WORDS = True\n",
    "OVERWRITE = False\n",
    "\n",
    "\n",
    "a = import_lemmatised_paragraphs(INPUT_DIR, POS, BATCHES, ONLY_ENGLISH_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9a7cd0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch': 'biggest_cities_20',\n",
       " 'original_paragraphs': 150,\n",
       " 'english_words': True}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['athens_bucharest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b42762cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21051"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pd.read_csv('../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___paris_london___/paris_london.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751b988e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "word_list_location = \"../../../../data/enwiki_city_pairs_lemmatised/lemmatised_paragraphs/\"\n",
    "city_pair_wordlists = []\n",
    "city_pairs = []\n",
    "for file in os.scandir(word_list_location):\n",
    "    with open(file.path, 'rb') as fp:\n",
    "        city_pair_wordlists.append(pickle.load(fp))\n",
    "        city_pairs.append(file.name.split('__')[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
