{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a80076dc",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3671f340",
   "metadata": {},
   "outputs": [],
   "source": [
    "from general_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "314358e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner', 'parser']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.disable_pipes('ner', 'parser')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7232e638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ADJ_CLEAN.pickle'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"ADJ.pickle\".split('.')\n",
    "'_CLEAN.'.join(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "15d4a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_path(path, OVERRIDE=False):\n",
    "    \"\"\"\n",
    "    -->\n",
    "        function that checks if file exists and if so, asks the user if they want to override it.\n",
    "\n",
    "        Parameters:\n",
    "            -----------\n",
    "                path: str -> path to check\n",
    "\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        if OVERRIDE:\n",
    "            print(f\"Overriding the following file: {path}\")\n",
    "            return True\n",
    "        else:\n",
    "            decision = input(f\"This following path already exists:\\n '{path}'\\nAre you sure you want to override?\\n Continue? [y/n]: \")\n",
    "            \n",
    "            if decision == 'y':\n",
    "                return True\n",
    "                print(\"The process has been continued.\")\n",
    "            elif decision == 'n':\n",
    "                print(\"The process has been halted.\")\n",
    "                return False\n",
    "            else:\n",
    "                print(\"You did not enter a valid option. \\nCanceling Operation...\")\n",
    "                return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d49e1f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise_paragraphs(df, OUTPUT_PATH, POS, ONLY_ENGLISH_WORDS=False, ENGLISH_WORD_LIST=[], OVERWRITE=False, NLP_MAX_LENGTH=1500000):\n",
    "    \"\"\"\n",
    "    -->\n",
    "        function that lemmatises the paragraphs of a single text file.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "            FILE_PATH: Str -> input directory path, to the text files\n",
    "            FILE_OUTPUT_DIR: Str -> output directory path, where you want to save the .pickle files\n",
    "            POS: string (e.g. \"NOUN\") -> options: (https://spacy.io/usage/spacy-101#annotations-pos-deps)\n",
    "            OVERRIDE_OLD_WORDLISTS: Bool -> Whether you want to override existing output files\n",
    "            NLP_MAX_LENGTH: Int (default: 1500000) -> Allowed number of characters per file\n",
    "    \"\"\"\n",
    "    \n",
    "    nlp.max_length = NLP_MAX_LENGTH\n",
    "    \n",
    "    #Checks if valid part-of-speech tag was provided\n",
    "    POStags=[\"PROPN\", \"AUX\", \"NOUN\", \"ADJ\", \"VERB\", \"ADP\", \"SYM\", \"NUM\"]\n",
    "    if not isinstance(POS, str) or POS.upper() not in POStags:\n",
    "        raise Exception(f'POSfilter only allows any of the following (SpaCy) part-of-speech tags: {POStags}.')\n",
    "    \n",
    "    paragraphs_dict = {}\n",
    "    if check_path(OUTPUT_PATH, OVERWRITE):\n",
    "        processed_paragraphs = [text for text in tqdm(nlp.pipe(df.paragraph, n_process=2, batch_size=1, disable=[\"ner\", \"parser\"]), desc=f\"Lemmatising ({OUTPUT_PATH.split('___')[1]})\",total=len(df.paragraph), leave=False)]\n",
    "        lemmatized_paragraphs = [[word.lemma_ for word in paragraph if word.pos_ == POS and not word.is_punct and not word.is_stop] for paragraph in processed_paragraphs]\n",
    "        regexed_paragraphs= [[re.sub(r'\\W+', '', word) for word in paragraph] for paragraph in lemmatized_paragraphs]\n",
    "        \n",
    "        for index, lemmatised_paragraph in enumerate(regexed_paragraphs):\n",
    "            paragraphs_dict[df.loc[index].paragraph_id] = lemmatised_paragraph\n",
    "\n",
    "        with open(OUTPUT_PATH, 'wb') as fp:\n",
    "            pickle.dump(paragraphs_dict, fp)\n",
    "    \n",
    "    filename = os.path.basename(OUTPUT_PATH)\n",
    "    CLEAN_PATH = f\"{os.path.dirname(OUTPUT_PATH)}/{'_CLEAN.'.join(filename.split('.'))}\"\n",
    "\n",
    "    if ONLY_ENGLISH_WORDS and check_path(CLEAN_PATH, OVERWRITE):\n",
    "        if not paragraphs_dict:\n",
    "            with open(OUTPUT_PATH, 'rb') as file_read:\n",
    "                    paragraphs_dict = pickle.load(file_read)\n",
    "                    \n",
    "        for paragraph_id in tqdm(paragraphs_dict, desc='Removing non-existent words', leave=False):\n",
    "            cleaned_lemmatised_paragraph = remove_non_existing_words(paragraphs_dict[paragraph_id], ENGLISH_WORD_LIST)\n",
    "            paragraphs_dict[paragraph_id] = cleaned_lemmatised_paragraph\n",
    "\n",
    "        with open(CLEAN_PATH, 'wb') as file_write:\n",
    "            pickle.dump(paragraphs_dict, file_write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8957d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english_word(word, english_words):\n",
    "    return word.lower() in english_words\n",
    "\n",
    "def remove_non_existing_words(wordlist: list, english_words) -> list:\n",
    "    if not len(english_words):\n",
    "        raise Exception(\"The supplied english words list is empty.\"\n",
    "                       )\n",
    "    wordset = set(wordlist)\n",
    "    non_existent = []\n",
    "    \n",
    "    for word in wordset:\n",
    "        if not is_english_word(word, english_words):\n",
    "            non_existent.append(word)\n",
    "            \n",
    "    return([word for word in wordlist if word not in non_existent])\n",
    "\n",
    "# def removing_non_existent_words(file_path, english_words):\n",
    "#     if not 'CLEAN' in file_path:\n",
    "#         with open(file_path, 'rb') as fp:\n",
    "#             lemmatised_paragraphs = pickle.load(fp)\n",
    "\n",
    "#             for paragraph_id in lemmatised_paragraphs:\n",
    "#                 cleaned_lemmatised_paragraph = remove_non_existing_words(lemmatised_paragraphs[paragraph_id], english_words)\n",
    "#                 lemmatised_paragraphs[paragraph_id] = cleaned_lemmatised_paragraph\n",
    "\n",
    "#         new_file_path = file_path.replace('.pickle, CLEAN.pickle')\n",
    "#         with open(new_file_path, 'wb') as fp:\n",
    "#             pickle.dump(lemmatised_paragraphs, fp)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aa20f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7b537d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_POS(POS):\n",
    "    POStags=[\"PROPN\", \"AUX\", \"NOUN\", \"ADJ\", \"VERB\", \"ADP\", \"SYM\", \"NUM\"]\n",
    "    \n",
    "    #Checks if valid part-of-speech tag was provided\n",
    "    if isinstance(POS, str):\n",
    "        if POS.upper() not in POStags:\n",
    "            raise Exception(f'POSfilter only allows any of the following (SpaCy) part-of-speech tags: {POStags}.')\n",
    "    elif isinstance(POS, list):\n",
    "        for tag in POS:\n",
    "            if tag.upper() not in POStags:\n",
    "                raise Exception(f'POSfilter only allows any of the following (SpaCy) part-of-speech tags: {POStags}.')\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "8231b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise(INPUT_DIR, POS, BATCHES=[], LEMMATISATION_TYPE='', ONLY_ENGLISH_WORDS=False, english_words_file=\"../../../input/english_words_alpha_370k.txt\", OVERWRITE=False):   \n",
    "    BATCHES = [int(x) for x in BATCHES]\n",
    "    reg_str = 'biggest_cities_([0-9]+)'\n",
    "    \n",
    "    #Checks if valid part-of-speech tag was provided\n",
    "    POStags = [\"PROPN\", \"AUX\", \"NOUN\", \"ADJ\", \"VERB\", \"ADP\", \"SYM\", \"NUM\"]\n",
    "    if not isinstance(POS, list) or len([tag.upper() for tag in POS if tag not in POStags]):\n",
    "        raise Exception(f'POSfilter only allows any of the following (SpaCy) part-of-speech tags: {POStags}.')\n",
    "    \n",
    "    if ONLY_ENGLISH_WORDS:\n",
    "        with open(english_words_file) as word_file:\n",
    "            ENGLISH_WORDS = set(word.strip().lower() for word in word_file)\n",
    "\n",
    "    batch_dirs = [os.path.join(INPUT_DIR, batch) for batch in os.listdir(INPUT_DIR) if not BATCHES or int(re.findall(reg_str, batch)[0]) in BATCHES]\n",
    "\n",
    "    # Where the magic happens\n",
    "    for batch_dir in tqdm(batch_dirs, desc=f\"BATCHES: {BATCHES}\"):\n",
    "        \n",
    "        for citypair in tqdm(os.listdir(batch_dir), desc=\"City Pair\", leave=False):\n",
    "            citypair_dir = os.path.join(batch_dir, citypair)\n",
    "            CITY_PAIR = citypair.split('___')[1]\n",
    "\n",
    "            df_paragraphs_path = f\"{citypair_dir}/{CITY_PAIR}.csv\"\n",
    "            if os.path.exists(df_paragraphs_path):\n",
    "                df = pd.read_csv(df_paragraphs_path)\n",
    "\n",
    "                for tag in tqdm(POS, desc=f\"POS: {POS}\", leave=False):\n",
    "                    POS_path = f\"{citypair_dir}/lemmatisation/{tag}.pickle\"\n",
    "                    lemmatise_paragraphs(df=df, \n",
    "                                         OUTPUT_PATH=POS_path,\n",
    "                                         POS=tag,\n",
    "                                         OVERWRITE=OVERWRITE,\n",
    "                                         ONLY_ENGLISH_WORDS=ONLY_ENGLISH_WORDS,\n",
    "                                         ENGLISH_WORD_LIST = ENGLISH_WORDS,\n",
    "                                         NLP_MAX_LENGTH=1500000)\n",
    "\n",
    "                    \n",
    "#             for foldertype in ['classification, frequencies', 'lemmatisation']:\n",
    "#                 os.makedirs(f\"{citypair_dir}/{foldertype}\", exist_ok=True)\n",
    "\n",
    "\n",
    "#             if lemmatisation:\n",
    "#                 for tag in POS:\n",
    "\n",
    "#                     FILE_PATH = os.path.join(root, file)\n",
    "#                     FILE_OUTPUT_DIR = root + '/' + 'lemmatisation'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "74eab495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33c49d9f85384ba79f5eb57c9745be7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BATCHES: [5]:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "City Pair:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (berlin_milan):   0%|          | 0/929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (berlin_milan):   0%|          | 0/929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (london_berlin):   0%|          | 0/7389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/7389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (london_berlin):   0%|          | 0/7389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/7389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (london_madrid):   0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (london_madrid):   0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (london_milan):   0%|          | 0/2512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/2512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (london_milan):   0%|          | 0/2512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/2512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (madrid_berlin):   0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (madrid_berlin):   0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (madrid_milan):   0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (madrid_milan):   0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_berlin):   0%|          | 0/7612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/7612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_berlin):   0%|          | 0/7612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/7612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_london):   0%|          | 0/21051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/21051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_london):   0%|          | 0/21051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/21051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_madrid):   0%|          | 0/3277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/3277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_madrid):   0%|          | 0/3277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/3277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['VERB', 'ADJ']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_milan):   0%|          | 0/3251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/3251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising (paris_milan):   0%|          | 0/3251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Removing non-existent words:   0%|          | 0/3251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 8s\n",
      "Wall time: 18min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "INPUT_DIR = \"../../../../../data/clean/city_pair_paragraphs/\"\n",
    "BATCHES = [5]\n",
    "POS = [\"VERB\", \"ADJ\"]\n",
    "# LEMMATISATION_TYPE = 'quick', 'accurate'\n",
    "ONLY_ENGLISH_WORDS = True\n",
    "OVERWRITE = False\n",
    "\n",
    "lemmatise(INPUT_DIR, POS, BATCHES, ONLY_ENGLISH_WORDS=ONLY_ENGLISH_WORDS, OVERWRITE=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "89b4f13f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___paris_london___/lemmatisation/NOUN_CLEAN.pickle'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [173]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m P \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___paris_london___/lemmatisation/NOUN_CLEAN.pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file_read:\n\u001b[0;32m      3\u001b[0m     paragraphs \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(file_read)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___paris_london___/lemmatisation/NOUN_CLEAN.pickle'"
     ]
    }
   ],
   "source": [
    "P = \"../../../../../data/clean/city_pair_paragraphs/biggest_cities_5/cities___paris_london___/lemmatisation/NOUN_CLEAN.pickle\"\n",
    "with open(P, 'rb') as file_read:\n",
    "    paragraphs = pickle.load(file_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "be593270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1749"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bc0a45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
