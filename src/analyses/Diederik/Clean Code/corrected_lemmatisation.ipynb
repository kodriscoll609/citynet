{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f2a9d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ner', 'parser']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp.disable_pipes('ner', 'parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "34cb957e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_english_words(path=\"../../../input/english_words_alpha_370k.txt\"):\n",
    "    if not os.path.exists(path):\n",
    "        raise Exception(\"Provide a valid path to a file with English words.\")\n",
    "\n",
    "    with open(path) as word_file:\n",
    "            ENGLISH_WORDS = set(word.strip().lower() for word in word_file)\n",
    "    \n",
    "    if not len(ENGLISH_WORDS):\n",
    "        raise Exception(\"No wordlist could be found in the given file!\")\n",
    "            \n",
    "    return ENGLISH_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d0385c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_english_word(word, english_words):\n",
    "    return word.lower() in english_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "836e6873",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_existing_words_from_wordlist(wordlist: list, english_words) -> list:\n",
    "    if not len(english_words):\n",
    "        raise Exception(\"The supplied english words list is empty.\"\n",
    "                       )\n",
    "    wordset = set(wordlist)\n",
    "    non_existent = []\n",
    "    \n",
    "    for word in wordset:\n",
    "        if not is_english_word(word, english_words):\n",
    "            non_existent.append(word)\n",
    "            \n",
    "    return([word for word in wordlist if word not in non_existent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a9bd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_english_words_in_paragraphs(paragraphs, english_words=[], english_words_file=\"../../../input/english_words_alpha_370k.txt\"):\n",
    "    if not english_words:\n",
    "        english_words = get_english_words(path=english_words_file)\n",
    "        \n",
    "    cleaned_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        cleaned_paragraph = remove_non_existing_words_from_wordlist(wordlist=paragraph, english_words=english_words)\n",
    "        cleaned_paragraphs.append(cleaned_paragraph)\n",
    "        \n",
    "    return cleaned_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "cf473af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise_paragraphs(paragraphs, POStag, NLP_MAX_LENGTH=1500000):\n",
    "    \"\"\"\n",
    "    --> function that lemmatises the paragraphs of a single text file.\n",
    "    \"\"\"\n",
    "    \n",
    "    nlp.max_length = NLP_MAX_LENGTH\n",
    "    \n",
    "    #Checks if valid part-of-speech tag was provided\n",
    "    POStags=[\"PROPN\", \"AUX\", \"NOUN\", \"ADJ\", \"VERB\", \"ADP\", \"SYM\", \"NUM\"]\n",
    "    if not isinstance(POStag, str) or POStag.upper() not in POStags:\n",
    "        raise Exception(f'POSfilter only allows any of the following (SpaCy) part-of-speech tags: {POStags}.')\n",
    "    \n",
    "    processed_paragraphs = [text for text in tqdm(nlp.pipe(paragraphs, n_process=2, batch_size=1, disable=[\"ner\", \"parser\"]), desc=f\"Lemmatising ({POStag})...\",total=len(paragraphs), leave=False)]\n",
    "    lemmatized_paragraphs = [[word.lemma_ for word in paragraph if word.pos_ == POStag and not word.is_punct and not word.is_stop] for paragraph in processed_paragraphs]\n",
    "    regexed_paragraphs= [[re.sub(r'\\W+', '', word) for word in paragraph] for paragraph in lemmatized_paragraphs]\n",
    "   \n",
    "    return regexed_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1dd57be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise_city_pair(df=df, POS=POS, OVERWRITE=False, ONLY_ENGLISH_WORDS=False, ENGLISH_WORDS = [],\n",
    "    english_words_file=\"../../../input/english_words_alpha_370k.txt\", NLP_MAX_LENGTH=1500000):\n",
    "    \n",
    "    for tag in tqdm(POS, desc=f\"POS: {POS}\", leave=False):\n",
    "        if OVERWRITE or tag not in df.columns:\n",
    "            df[f\"{tag}\"] = lemmatise_paragraphs(paragraphs=df['paragraph'], POStag=tag, NLP_MAX_LENGTH=NLP_MAX_LENGTH)\n",
    "\n",
    "        if ONLY_ENGLISH_WORDS and (OVERWRITE or f'{tag}_clean' not in df.columns):\n",
    "            df[f'{tag}_clean'] = keep_english_words_in_paragraphs(paragraphs=df[tag], english_words=ENGLISH_WORDS)\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "e026f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatise(INPUT_DIR, POS, BATCHES=[], LEMMATISATION_TYPE='', ONLY_ENGLISH_WORDS=False, english_words_file=\"../../../input/english_words_alpha_370k.txt\", OVERWRITE=False, NLP_MAX_LENGTH=1500000):   \n",
    "    BATCHES = [str(batch) for batch in BATCHES]\n",
    "    \n",
    "    #Checks if valid part-of-speech tag was provided\n",
    "    POStags = [\"PROPN\", \"AUX\", \"NOUN\", \"ADJ\", \"VERB\", \"ADP\", \"SYM\", \"NUM\"]\n",
    "    if not isinstance(POS, list) or len([tag.upper() for tag in POS if tag not in POStags]):\n",
    "        raise Exception(f'POSfilter only allows any of the following (SpaCy) part-of-speech tags: {POStags}.')\n",
    "    \n",
    "    if ONLY_ENGLISH_WORDS:\n",
    "        with open(english_words_file) as word_file:\n",
    "            ENGLISH_WORDS = set(word.strip().lower() for word in word_file)\n",
    "\n",
    "    chosen_batches = [batch for batch in os.listdir(INPUT_DIR) if not BATCHES or batch in BATCHES]\n",
    "    \n",
    "#     # Where the magic happens\n",
    "    for batch in tqdm(chosen_batches, desc=f\"BATCHES: {BATCHES}\"):\n",
    "        batch_dir = os.path.join(INPUT_DIR, batch)\n",
    "        \n",
    "        for citypair in tqdm(os.listdir(batch_dir), desc=\"City Pair\", leave=False):\n",
    "            citypair_dir = os.path.join(batch_dir, citypair)\n",
    "            CITY_PAIR = citypair.split('___')[1]\n",
    "\n",
    "            df_paragraphs_path = f\"{citypair_dir}/{CITY_PAIR}.csv\"\n",
    "            \n",
    "            if os.path.exists(df_paragraphs_path):\n",
    "                df = pd.read_csv(df_paragraphs_path)\n",
    "                df = lemmatise_city_pair(df=df, POS=POS, OVERWRITE=OVERWRITE, ONLY_ENGLISH_WORDS=ONLY_ENGLISH_WORDS, ENGLISH_WORDS=ENGLISH_WORDS, NLP_MAX_LENGTH=NLP_MAX_LENGTH)\n",
    "                df.to_csv(df_paragraphs_path, index=False)\n",
    "            else:\n",
    "                print(f\"Batch: {batch}, City Pair: '{CITY_PAIR}' has no file at '{df_paragraphs_path}'.\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9fb645ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6ee2b8a5464bd79b8c6b06895c45ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BATCHES: ['5']:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "City Pair:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['NOUN', 'VERB']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/929 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['NOUN', 'VERB']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/7389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/7389 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['NOUN', 'VERB']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/2313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['NOUN', 'VERB']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/2512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/2512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['NOUN', 'VERB']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/1034 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['NOUN', 'VERB']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/1128 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['NOUN', 'VERB']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/7612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/7612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['NOUN', 'VERB']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/21051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/21051 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['NOUN', 'VERB']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/3277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/3277 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "POS: ['NOUN', 'VERB']:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/3251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Lemmatising...:   0%|          | 0/3251 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 4s\n",
      "Wall time: 20min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "INPUT_DIR = \"../../../../../data/clean/city_pair_paragraphs3/\"\n",
    "BATCHES = [5]\n",
    "POS = [\"NOUN\", \"VERB\"]\n",
    "# LEMMATISATION_TYPE = 'quick', 'accurate'\n",
    "ONLY_ENGLISH_WORDS = True\n",
    "OVERWRITE = True\n",
    "\n",
    "df = lemmatise(INPUT_DIR, POS, BATCHES, ONLY_ENGLISH_WORDS=ONLY_ENGLISH_WORDS, OVERWRITE=OVERWRITE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f590342e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Title', 'city_pair', 'article_id', 'paragraph_id', 'paragraph'], dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f9ab722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm_df = pd.read_csv(\"../../../../../data/clean/city_pair_paragraphs3/5/cities___berlin_milan___/berlin_milan.csv\", converters={'ADJ': literal_eval})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7e52a3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "columns_to_convert = ['']\n",
    "for column in columns_to_convert:\n",
    "    try:\n",
    "        bm_df.column = bm_df.column.apply(literal_eval)\n",
    "    except:\n",
    "        print(\"Could not convert column values to lists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5ebb07b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>city_pair</th>\n",
       "      <th>article_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>paragraph</th>\n",
       "      <th>ADJ</th>\n",
       "      <th>ADJ_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>arthur schopenhauer</td>\n",
       "      <td>berlin_milan</td>\n",
       "      <td>700</td>\n",
       "      <td>1</td>\n",
       "      <td>after his tenure in academia, he continued to ...</td>\n",
       "      <td>[old, unknown, paralyzed, right, unable, guilt...</td>\n",
       "      <td>['old', 'unknown', 'paralyzed', 'right', 'unab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>asteroid</td>\n",
       "      <td>berlin_milan</td>\n",
       "      <td>791</td>\n",
       "      <td>2</td>\n",
       "      <td>one of the astronomers selected for the search...</td>\n",
       "      <td>[catholic, 87th, zodiacal, like, final, fellow...</td>\n",
       "      <td>['catholic', 'zodiacal', 'like', 'final', 'fel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>transport in armenia</td>\n",
       "      <td>berlin_milan</td>\n",
       "      <td>1096</td>\n",
       "      <td>3</td>\n",
       "      <td>there are plenty of air connections between ye...</td>\n",
       "      <td>[regional, daily, major, main, yearly]</td>\n",
       "      <td>['regional', 'daily', 'major', 'main', 'yearly']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>barcelona</td>\n",
       "      <td>berlin_milan</td>\n",
       "      <td>4443</td>\n",
       "      <td>4</td>\n",
       "      <td>since 2009, 'the brandery', an urban fashion s...</td>\n",
       "      <td>[urban, global, annual, seventh, important]</td>\n",
       "      <td>['urban', 'global', 'annual', 'seventh', 'impo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>europe</td>\n",
       "      <td>berlin_milan</td>\n",
       "      <td>9239</td>\n",
       "      <td>5</td>\n",
       "      <td>when considering the commuter belts or metropo...</td>\n",
       "      <td>[metropolitan, comparable, available, large]</td>\n",
       "      <td>['metropolitan', 'comparable', 'available', 'l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>924</th>\n",
       "      <td>caspar voght</td>\n",
       "      <td>berlin_milan</td>\n",
       "      <td>3878145</td>\n",
       "      <td>933</td>\n",
       "      <td>at the age of 12, caspar voght fell seriously ...</td>\n",
       "      <td>[permanent, facial, inclined, little, grand, s...</td>\n",
       "      <td>['permanent', 'facial', 'inclined', 'little', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>925</th>\n",
       "      <td>hasmik papian</td>\n",
       "      <td>berlin_milan</td>\n",
       "      <td>3909402</td>\n",
       "      <td>934</td>\n",
       "      <td>after her debut at the armenian national opera...</td>\n",
       "      <td>[national, international, numerous, prestigiou...</td>\n",
       "      <td>['national', 'international', 'numerous', 'pre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926</th>\n",
       "      <td>michèle crider</td>\n",
       "      <td>berlin_milan</td>\n",
       "      <td>3936952</td>\n",
       "      <td>935</td>\n",
       "      <td>since 1991, crider has been heard regularly in...</td>\n",
       "      <td>[great, metropolitan, successful, orange, nati...</td>\n",
       "      <td>['great', 'metropolitan', 'successful', 'orang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927</th>\n",
       "      <td>surendranath dasgupta</td>\n",
       "      <td>berlin_milan</td>\n",
       "      <td>3939717</td>\n",
       "      <td>936</td>\n",
       "      <td>the impressions that he had made by his speech...</td>\n",
       "      <td>[international, illuminated, second]</td>\n",
       "      <td>['international', 'illuminated', 'second']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>928</th>\n",
       "      <td>hennes weisweiler</td>\n",
       "      <td>berlin_milan</td>\n",
       "      <td>3953677</td>\n",
       "      <td>937</td>\n",
       "      <td>the team's first european champions cup partic...</td>\n",
       "      <td>[second, second, celebrated, roberto, soft, in...</td>\n",
       "      <td>['second', 'second', 'celebrated', 'roberto', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>929 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Title     city_pair  article_id  paragraph_id  \\\n",
       "0      arthur schopenhauer  berlin_milan         700             1   \n",
       "1                 asteroid  berlin_milan         791             2   \n",
       "2     transport in armenia  berlin_milan        1096             3   \n",
       "3                barcelona  berlin_milan        4443             4   \n",
       "4                   europe  berlin_milan        9239             5   \n",
       "..                     ...           ...         ...           ...   \n",
       "924           caspar voght  berlin_milan     3878145           933   \n",
       "925          hasmik papian  berlin_milan     3909402           934   \n",
       "926         michèle crider  berlin_milan     3936952           935   \n",
       "927  surendranath dasgupta  berlin_milan     3939717           936   \n",
       "928      hennes weisweiler  berlin_milan     3953677           937   \n",
       "\n",
       "                                             paragraph  \\\n",
       "0    after his tenure in academia, he continued to ...   \n",
       "1    one of the astronomers selected for the search...   \n",
       "2    there are plenty of air connections between ye...   \n",
       "3    since 2009, 'the brandery', an urban fashion s...   \n",
       "4    when considering the commuter belts or metropo...   \n",
       "..                                                 ...   \n",
       "924  at the age of 12, caspar voght fell seriously ...   \n",
       "925  after her debut at the armenian national opera...   \n",
       "926  since 1991, crider has been heard regularly in...   \n",
       "927  the impressions that he had made by his speech...   \n",
       "928  the team's first european champions cup partic...   \n",
       "\n",
       "                                                   ADJ  \\\n",
       "0    [old, unknown, paralyzed, right, unable, guilt...   \n",
       "1    [catholic, 87th, zodiacal, like, final, fellow...   \n",
       "2               [regional, daily, major, main, yearly]   \n",
       "3          [urban, global, annual, seventh, important]   \n",
       "4         [metropolitan, comparable, available, large]   \n",
       "..                                                 ...   \n",
       "924  [permanent, facial, inclined, little, grand, s...   \n",
       "925  [national, international, numerous, prestigiou...   \n",
       "926  [great, metropolitan, successful, orange, nati...   \n",
       "927               [international, illuminated, second]   \n",
       "928  [second, second, celebrated, roberto, soft, in...   \n",
       "\n",
       "                                             ADJ_clean  \n",
       "0    ['old', 'unknown', 'paralyzed', 'right', 'unab...  \n",
       "1    ['catholic', 'zodiacal', 'like', 'final', 'fel...  \n",
       "2     ['regional', 'daily', 'major', 'main', 'yearly']  \n",
       "3    ['urban', 'global', 'annual', 'seventh', 'impo...  \n",
       "4    ['metropolitan', 'comparable', 'available', 'l...  \n",
       "..                                                 ...  \n",
       "924  ['permanent', 'facial', 'inclined', 'little', ...  \n",
       "925  ['national', 'international', 'numerous', 'pre...  \n",
       "926  ['great', 'metropolitan', 'successful', 'orang...  \n",
       "927         ['international', 'illuminated', 'second']  \n",
       "928  ['second', 'second', 'celebrated', 'roberto', ...  \n",
       "\n",
       "[929 rows x 7 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "101892a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "malformed node or string: ['old', 'unknown', 'paralyzed', 'right', 'unable', 'guilty', 'annual']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [124]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bm_df\u001b[38;5;241m.\u001b[39mADJ_clean \u001b[38;5;241m=\u001b[39m \u001b[43mbm_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mADJ_clean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mliteral_eval\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Personal Files [Local]\\Applied Data Science\\Thesis - CITYNET\\venv_citynet3\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Personal Files [Local]\\Applied Data Science\\Thesis - CITYNET\\venv_citynet3\\lib\\site-packages\\pandas\\core\\apply.py:1082\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1082\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Personal Files [Local]\\Applied Data Science\\Thesis - CITYNET\\venv_citynet3\\lib\\site-packages\\pandas\\core\\apply.py:1137\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1132\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1133\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1135\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1136\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1137\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1144\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\Personal Files [Local]\\Applied Data Science\\Thesis - CITYNET\\venv_citynet3\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ast.py:105\u001b[0m, in \u001b[0;36mliteral_eval\u001b[1;34m(node_or_string)\u001b[0m\n\u001b[0;32m    103\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_or_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ast.py:104\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    103\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[1;32m--> 104\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_signed_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ast.py:78\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_signed_num\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m operand\n\u001b[1;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ast.py:69\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_num\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_num\u001b[39m(node):\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Constant) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m):\n\u001b[1;32m---> 69\u001b[0m         \u001b[43m_raise_malformed_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ast.py:66\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._raise_malformed_node\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_malformed_node\u001b[39m(node):\n\u001b[1;32m---> 66\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmalformed node or string: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: malformed node or string: ['old', 'unknown', 'paralyzed', 'right', 'unable', 'guilty', 'annual']"
     ]
    }
   ],
   "source": [
    "bm_df.ADJ_clean = bm_df.ADJ_clean.apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "de6c8c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'87th'}\n",
      "{'rossinian'}\n",
      "{'68th'}\n",
      "{'austro'}\n",
      "{'operabase', 'calaf'}\n",
      "{'87th'}\n",
      "{'wizz'}\n",
      "{'edizioni'}\n",
      "{'eldoret'}\n",
      "{'andré', 'opéra'}\n",
      "{'tiergarten'}\n",
      "{'legnanese', 'brianza', 'bustocco'}\n",
      "{'upmarket'}\n",
      "{'firelands'}\n",
      "{'madama'}\n",
      "{'10th', 'euwe'}\n",
      "{'opéra'}\n",
      "{'ailey', '16th', '50th', '1st'}\n",
      "{'fckh8'}\n",
      "{'19961997'}\n",
      "{'duetten'}\n",
      "{'treybal', '15th', '14th', '2nd'}\n",
      "{'voix', 'caramoor'}\n",
      "{'lichbild'}\n",
      "{'madama'}\n",
      "{'philarmonic'}\n",
      "{'click4sky'}\n",
      "{'1st'}\n",
      "{'walküre'}\n",
      "{'mašín'}\n",
      "{'janin', 'graeco'}\n",
      "{'grzegorz'}\n",
      "{'neue'}\n",
      "{'imcs', '7th'}\n",
      "{'micropolitan'}\n",
      "{'18th', '19th'}\n",
      "{'viennas', 'hungarys'}\n",
      "{'9th'}\n",
      "{'wessenberg'}\n",
      "{'20th', '19th'}\n",
      "{'opéra'}\n",
      "{'18th', 'kindl'}\n",
      "{'järvi', 'rundfunk', 'sinfonieorchester', 'inbal'}\n",
      "{'calaf'}\n",
      "{'stokla'}\n",
      "{'bloblike', 'doomy'}\n",
      "{'monteggia'}\n",
      "{'22nd'}\n",
      "{'zandonai'}\n",
      "{'opéra'}\n",
      "{'václav', 'saarbrücken', 'gerd'}\n",
      "{'cremonese'}\n",
      "{'custo'}\n",
      "{'petipa'}\n",
      "{'groundbreaking'}\n",
      "{'nur'}\n",
      "{'mouvement', '1st'}\n",
      "{'jiří', 'ladislav'}\n",
      "{'nanki'}\n",
      "{'opéra'}\n",
      "{'ernani'}\n",
      "{'mtsensk'}\n",
      "{'passante'}\n",
      "{'6th'}\n",
      "{'leipsic'}\n",
      "{'18th'}\n",
      "{'gdr'}\n",
      "{'jhochhen'}\n",
      "{'eine'}\n",
      "{'5th'}\n",
      "{'antonov'}\n",
      "{'mediatic'}\n",
      "{'deutscher'}\n",
      "{'15th'}\n",
      "{'18th', '17th', '12th', '19th'}\n",
      "{'21st'}\n",
      "{'brs'}\n",
      "{'stewartstown'}\n",
      "{'silmo'}\n",
      "{'transdisciplinary'}\n",
      "{'2nd'}\n",
      "{'lutosławski', 'nieuw', 'aleksandar'}\n",
      "{'montauban', 'ouen'}\n",
      "{'sauret'}\n",
      "{'šipuš', 'nikša'}\n",
      "{'awara'}\n",
      "{'20th'}\n",
      "{'kuurne', 'nokere'}\n",
      "{'altes'}\n",
      "{'neue'}\n",
      "{'trivulzio'}\n",
      "{'conjuntural'}\n",
      "{'gallerist'}\n",
      "{'qt8'}\n",
      "{'750th'}\n",
      "{'opéra'}\n",
      "{'brünnhilde'}\n",
      "{'mašín'}\n",
      "{'200th'}\n",
      "{'lappo'}\n",
      "{'21st'}\n",
      "{'bienal'}\n",
      "{'bahn'}\n",
      "{'opéra'}\n",
      "{'antonov', 'donbassaero'}\n",
      "{'poliuto'}\n",
      "{'48th', 'goldene', 'telegatto', 'madama'}\n",
      "{'greuther'}\n",
      "{'33rd', '3rd', '4th', '9th'}\n",
      "{'daimler'}\n",
      "{'jemolo'}\n",
      "{'guggenheim'}\n",
      "{'goldene'}\n",
      "{'usbased', 'getyourguide'}\n",
      "{'musicali'}\n",
      "{'norina'}\n",
      "{'ludovic'}\n",
      "{'acsa'}\n",
      "{'anthonie'}\n",
      "{'monsù'}\n",
      "{'sido'}\n",
      "{'giessen'}\n",
      "{'revi'}\n",
      "{'aktuelle', 'für'}\n",
      "{'10th'}\n",
      "{'stachu'}\n",
      "{'für'}\n",
      "{'16th'}\n",
      "{'saarbrücken', 'erste'}\n",
      "{'3d'}\n",
      "{'razstava'}\n",
      "{'20th', 'jevrem', 'nahiye', 'jaša'}\n",
      "{'8th', 'deruta', '1st'}\n",
      "{'madama'}\n",
      "{'gagnidze', 'opéra'}\n",
      "{'nationaltheater', 'lupupa'}\n",
      "{'4th'}\n",
      "{'13th'}\n",
      "{'dragomir'}\n",
      "{'sprüth', 'rachofsky'}\n",
      "{'49th'}\n",
      "{'neues'}\n",
      "{'200th', '60th'}\n",
      "{'180th', 'walküre'}\n",
      "{'domplatz', 'wechselbank', '19th'}\n",
      "{'muti', 'salò'}\n",
      "{'193'}\n",
      "{'aleksei'}\n",
      "{'opéra'}\n",
      "{'stettin'}\n",
      "{'seddin'}\n",
      "{'noch', '3rd', 'eine', '10th', 'kunstsammlung'}\n",
      "{'lizi', 'duzès', 'riquier'}\n",
      "{'brodinski'}\n",
      "{'199798'}\n",
      "{'opéra'}\n",
      "{'neumayer'}\n",
      "{'àlex'}\n",
      "{'für'}\n",
      "{'staatsoper', 'opéra'}\n",
      "{'nasci', '150th'}\n",
      "{'15th', '31st'}\n",
      "{'schönen'}\n",
      "{'staatsoper', 'vedo'}\n",
      "{'madama'}\n",
      "{'großes'}\n",
      "{'ustashi'}\n",
      "{'hochschule'}\n",
      "{'verdean', 'arrigo'}\n",
      "{'nationaltheater'}\n",
      "{'abigaille'}\n",
      "{'italo'}\n",
      "{'freie'}\n",
      "{'esade'}\n",
      "{'elaste', '32nd'}\n",
      "{'15th', 'alte', '1st', 'μοzaik'}\n",
      "{'ghiringhelli'}\n",
      "{'irmgard'}\n",
      "{'20th'}\n",
      "{'austro'}\n",
      "{'2nd'}\n",
      "{'hochschule', 'staatsoper'}\n",
      "{'opéra'}\n",
      "{'114th'}\n",
      "{'alte', 'neue'}\n",
      "{'19th'}\n",
      "{'hammerskin'}\n",
      "{'slavenka'}\n",
      "{'20th'}\n",
      "{'hochschule'}\n",
      "{'sänger'}\n",
      "{'dramani'}\n",
      "{'velodromo'}\n",
      "{'mozartian'}\n",
      "{'émigré'}\n",
      "{'václav', '60th', '84th', '70th'}\n",
      "{'ristić'}\n",
      "{'nordio'}\n",
      "{'43rd'}\n",
      "{'indie', 'sarasate', 'ormandy'}\n",
      "{'12th'}\n",
      "{'grieta'}\n",
      "{'šapina', '80th'}\n",
      "{'carrà'}\n",
      "{'standalone'}\n",
      "{'für', 'jahrbuch'}\n",
      "{'18th'}\n",
      "{'julijana'}\n"
     ]
    }
   ],
   "source": [
    "# bm_df.ADJ_clean = bm_df.ADJ_clean.apply(literal_eval)\n",
    "\n",
    "for index, row in bm_df.iterrows():\n",
    "    if len(row.ADJ) != len(row.ADJ_clean):\n",
    "        #print(type(row.ADJ_clean))\n",
    "        print(set(row.ADJ).difference(set(row.ADJ_clean))) #, '\\n', row.ADJ_clean)\n",
    "    #print('-->', lenrow.ADJ, '\\n==', row.ADJ_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204e3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
